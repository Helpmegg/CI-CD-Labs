
==> Audit <==
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ COMMAND â”‚ ARGS â”‚ PROFILE  â”‚ USER  â”‚ VERSION â”‚     START TIME      â”‚      END TIME       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ start   â”‚      â”‚ minikube â”‚ denys â”‚ v1.37.0 â”‚ 18 Nov 25 11:42 UTC â”‚                     â”‚
â”‚ start   â”‚      â”‚ minikube â”‚ denys â”‚ v1.37.0 â”‚ 18 Nov 25 12:15 UTC â”‚                     â”‚
â”‚ start   â”‚      â”‚ minikube â”‚ denys â”‚ v1.37.0 â”‚ 18 Nov 25 12:16 UTC â”‚ 18 Nov 25 12:18 UTC â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


==> Last Start <==
Log file created at: 2025/11/18 12:16:30
Running on machine: Denys
Binary: Built with gc go1.24.6 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1118 12:16:30.940572   10147 out.go:360] Setting OutFile to fd 1 ...
I1118 12:16:30.940907   10147 out.go:413] isatty.IsTerminal(1) = true
I1118 12:16:30.940910   10147 out.go:374] Setting ErrFile to fd 2...
I1118 12:16:30.940913   10147 out.go:413] isatty.IsTerminal(2) = true
I1118 12:16:30.941227   10147 root.go:338] Updating PATH: /home/denys/.minikube/bin
W1118 12:16:30.941336   10147 root.go:314] Error reading config file at /home/denys/.minikube/config/config.json: open /home/denys/.minikube/config/config.json: no such file or directory
I1118 12:16:30.941704   10147 out.go:368] Setting JSON to false
I1118 12:16:30.942744   10147 start.go:130] hostinfo: {"hostname":"Denys","uptime":3438,"bootTime":1763464753,"procs":225,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"24.04","kernelVersion":"6.14.0-35-generic","kernelArch":"x86_64","virtualizationSystem":"vbox","virtualizationRole":"guest","hostId":"3612af41-f115-43e8-a675-34be7ef7df69"}
I1118 12:16:30.942814   10147 start.go:140] virtualization: vbox guest
I1118 12:16:30.949875   10147 out.go:179] ðŸ˜„  minikube v1.37.0 on Ubuntu 24.04 (vbox/amd64)
I1118 12:16:30.954925   10147 driver.go:421] Setting default libvirt URI to qemu:///system
I1118 12:16:30.954962   10147 global.go:112] Querying for installed drivers using PATH=/home/denys/.minikube/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin:/snap/bin
W1118 12:16:30.955736   10147 preload.go:293] Failed to list preload files: open /home/denys/.minikube/cache/preloaded-tarball: no such file or directory
I1118 12:16:30.955779   10147 notify.go:220] Checking for updates...
I1118 12:16:31.029638   10147 docker.go:123] docker version: linux-29.0.2:Docker Engine - Community
I1118 12:16:31.029750   10147 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1118 12:16:31.193696   10147 info.go:266] docker info: {ID:34196bf2-ca57-48f8-9574-71287b501b37 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:21 OomKillDisable:false NGoroutines:46 SystemTime:2025-11-18 12:16:31.157643104 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-35-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8 ::1/128] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:7838707712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Denys Labels:[] ExperimentalBuild:false ServerVersion:29.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.30.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3]] Warnings:<nil>}}
I1118 12:16:31.194022   10147 docker.go:318] overlay module found
I1118 12:16:31.194299   10147 global.go:133] docker default: true priority: 9, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1118 12:16:31.229896   10147 global.go:133] none default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1118 12:16:31.230447   10147 global.go:133] podman default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "podman": executable file not found in $PATH Reason: Fix:Install Podman Doc:https://minikube.sigs.k8s.io/docs/drivers/podman/ Version:}
I1118 12:16:31.230483   10147 global.go:133] ssh default: false priority: 4, state: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1118 12:16:31.230546   10147 global.go:133] kvm2 default: true priority: 8, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "virsh": executable file not found in $PATH Reason: Fix:Install libvirt Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/kvm2/ Version:}
I1118 12:16:31.230605   10147 global.go:133] qemu2 default: true priority: 7, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "qemu-system-x86_64": executable file not found in $PATH Reason: Fix:Install qemu-system Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/qemu/ Version:}
I1118 12:16:31.230679   10147 global.go:133] virtualbox default: true priority: 6, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:unable to find VBoxManage in $PATH Reason: Fix:Install VirtualBox Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/virtualbox/ Version:}
I1118 12:16:31.230715   10147 global.go:133] vmware default: false priority: 5, state: {Installed:false Healthy:false Running:false NeedsImprovement:false Error:exec: "vmrun": executable file not found in $PATH Reason: Fix:Install vmrun Doc:https://minikube.sigs.k8s.io/docs/reference/drivers/vmware/ Version:}
I1118 12:16:31.230737   10147 driver.go:343] not recommending "none" due to default: false
I1118 12:16:31.230741   10147 driver.go:343] not recommending "ssh" due to default: false
I1118 12:16:31.230755   10147 driver.go:378] Picked: docker
I1118 12:16:31.230762   10147 driver.go:379] Alternatives: [none ssh]
I1118 12:16:31.230768   10147 driver.go:380] Rejects: [podman kvm2 qemu2 virtualbox vmware]
I1118 12:16:31.237071   10147 out.go:179] âœ¨  Automatically selected the docker driver. Other choices: none, ssh
I1118 12:16:31.241935   10147 start.go:304] selected driver: docker
I1118 12:16:31.241951   10147 start.go:918] validating driver "docker" against <nil>
I1118 12:16:31.241967   10147 start.go:929] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1118 12:16:31.242115   10147 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1118 12:16:31.360062   10147 info.go:266] docker info: {ID:34196bf2-ca57-48f8-9574-71287b501b37 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:21 OomKillDisable:false NGoroutines:46 SystemTime:2025-11-18 12:16:31.335526861 +0000 UTC LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.14.0-35-generic OperatingSystem:Ubuntu 24.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8 ::1/128] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:2 MemTotal:7838707712 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:Denys Labels:[] ExperimentalBuild:false ServerVersion:29.0.2 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:fcd43222d6b07379a4be9786bda52438f0dd16a1 Expected:} RuncCommit:{ID:v1.3.3-0-gd842d771 Expected:} InitCommit:{ID:de40ad0 Expected:} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.30.0] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.40.3]] Warnings:<nil>}}
I1118 12:16:31.360180   10147 start_flags.go:327] no existing cluster config was found, will generate one from the flags 
I1118 12:16:31.360420   10147 start_flags.go:410] Using suggested 3072MB memory alloc based on sys=7475MB, container=7475MB
I1118 12:16:31.360551   10147 start_flags.go:974] Wait components to verify : map[apiserver:true system_pods:true]
I1118 12:16:31.365678   10147 out.go:179] ðŸ“Œ  Using Docker driver with root privileges
I1118 12:16:31.370012   10147 cni.go:84] Creating CNI manager for ""
I1118 12:16:31.370110   10147 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1118 12:16:31.370120   10147 start_flags.go:336] Found "bridge CNI" CNI - setting NetworkPlugin=cni
I1118 12:16:31.370245   10147 start.go:348] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1118 12:16:31.375456   10147 out.go:179] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I1118 12:16:31.381799   10147 cache.go:123] Beginning downloading kic base image for docker with docker
I1118 12:16:31.389115   10147 out.go:179] ðŸšœ  Pulling base image v0.0.48 ...
I1118 12:16:31.394727   10147 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 12:16:31.394909   10147 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local docker daemon
I1118 12:16:31.612667   10147 cache.go:152] Downloading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1118 12:16:31.613222   10147 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 in local cache directory
I1118 12:16:31.613744   10147 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 to local cache
I1118 12:16:31.618706   10147 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1118 12:16:31.618723   10147 cache.go:58] Caching tarball of preloaded images
I1118 12:16:31.618976   10147 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 12:16:31.625359   10147 out.go:179] ðŸ’¾  Downloading Kubernetes v1.34.0 preload ...
I1118 12:16:31.628533   10147 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1118 12:16:31.844591   10147 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.34.0/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4?checksum=md5:994a4de1464928e89c992dfd0a962e35 -> /home/denys/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4
I1118 12:17:04.498484   10147 cache.go:155] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 as a tarball
I1118 12:17:04.498505   10147 cache.go:165] Loading gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from local cache
I1118 12:17:10.509349   10147 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1118 12:17:10.509468   10147 preload.go:254] verifying checksum of /home/denys/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4 ...
I1118 12:17:12.550957   10147 cache.go:61] Finished verifying existence of preloaded tar for v1.34.0 on docker
I1118 12:17:12.552020   10147 profile.go:143] Saving config to /home/denys/.minikube/profiles/minikube/config.json ...
I1118 12:17:12.552125   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/profiles/minikube/config.json: {Name:mkbfe5c2bf16222cdb3f932ae1e448fe718efc57 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:17:40.086755   10147 cache.go:167] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 from cached tarball
I1118 12:17:40.086919   10147 cache.go:232] Successfully downloaded all kic artifacts
I1118 12:17:40.086972   10147 start.go:360] acquireMachinesLock for minikube: {Name:mk242af4905a94f78cfd3030b61ea9e5479f2101 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1118 12:17:40.087474   10147 start.go:364] duration metric: took 173.003Âµs to acquireMachinesLock for "minikube"
I1118 12:17:40.088749   10147 start.go:93] Provisioning new machine with config: &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} &{Name: IP: Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1118 12:17:40.088844   10147 start.go:125] createHost starting for "" (driver="docker")
I1118 12:17:40.097147   10147 out.go:252] ðŸ”¥  Creating docker container (CPUs=2, Memory=3072MB) ...
I1118 12:17:40.098508   10147 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I1118 12:17:40.098535   10147 client.go:168] LocalClient.Create starting
I1118 12:17:40.101791   10147 main.go:141] libmachine: Creating CA: /home/denys/.minikube/certs/ca.pem
I1118 12:17:40.191790   10147 main.go:141] libmachine: Creating client certificate: /home/denys/.minikube/certs/cert.pem
I1118 12:17:40.357434   10147 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W1118 12:17:40.403936   10147 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I1118 12:17:40.404010   10147 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I1118 12:17:40.404025   10147 cli_runner.go:164] Run: docker network inspect minikube
W1118 12:17:40.427953   10147 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I1118 12:17:40.427975   10147 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I1118 12:17:40.427988   10147 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I1118 12:17:40.428308   10147 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1118 12:17:40.471527   10147 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc001412990}
I1118 12:17:40.471575   10147 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I1118 12:17:40.471626   10147 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I1118 12:17:41.420046   10147 network_create.go:108] docker network minikube 192.168.49.0/24 created
I1118 12:17:41.420072   10147 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I1118 12:17:41.420130   10147 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I1118 12:17:41.507851   10147 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I1118 12:17:41.566919   10147 oci.go:103] Successfully created a docker volume minikube
I1118 12:17:41.566984   10147 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib
I1118 12:17:44.413485   10147 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -d /var/lib: (2.846467598s)
I1118 12:17:44.413505   10147 oci.go:107] Successfully prepared a docker volume minikube
I1118 12:17:44.413568   10147 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 12:17:44.413590   10147 kic.go:194] Starting extracting preloaded images to volume ...
I1118 12:17:44.413641   10147 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v /home/denys/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir
I1118 12:17:49.779917   10147 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v /home/denys/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.34.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 -I lz4 -xf /preloaded.tar -C /extractDir: (5.366233279s)
I1118 12:17:49.779946   10147 kic.go:203] duration metric: took 5.366351859s to extract preloaded images to volume ...
W1118 12:17:49.780067   10147 cgroups_linux.go:77] Your kernel does not support swap limit capabilities or the cgroup is not mounted.
W1118 12:17:49.780107   10147 oci.go:252] Your kernel does not support CPU cfs period/quota or the cgroup is not mounted.
I1118 12:17:49.780424   10147 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I1118 12:17:50.603087   10147 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3072mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1
I1118 12:17:51.989775   10147 cli_runner.go:217] Completed: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=3072mb -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1: (1.386474857s)
I1118 12:17:51.990004   10147 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I1118 12:17:52.050548   10147 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 12:17:52.114782   10147 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I1118 12:17:52.276926   10147 oci.go:144] the created container "minikube" has a running status.
I1118 12:17:52.277343   10147 kic.go:225] Creating ssh key for kic: /home/denys/.minikube/machines/minikube/id_rsa...
I1118 12:17:52.575011   10147 kic_runner.go:191] docker (temp): /home/denys/.minikube/machines/minikube/id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I1118 12:17:52.672566   10147 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 12:17:52.719388   10147 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I1118 12:17:52.719486   10147 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I1118 12:17:52.827534   10147 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 12:17:52.883660   10147 machine.go:93] provisionDockerMachine start ...
I1118 12:17:52.883767   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:17:52.950424   10147 main.go:141] libmachine: Using SSH client type: native
I1118 12:17:52.950654   10147 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 12:17:52.950660   10147 main.go:141] libmachine: About to run SSH command:
hostname
I1118 12:17:52.953598   10147 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:37476->127.0.0.1:32768: read: connection reset by peer
I1118 12:17:56.244946   10147 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1118 12:17:56.244976   10147 ubuntu.go:182] provisioning hostname "minikube"
I1118 12:17:56.245656   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:17:56.340118   10147 main.go:141] libmachine: Using SSH client type: native
I1118 12:17:56.341003   10147 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 12:17:56.341016   10147 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1118 12:17:56.702832   10147 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1118 12:17:56.703175   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:17:56.826373   10147 main.go:141] libmachine: Using SSH client type: native
I1118 12:17:56.828840   10147 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 12:17:56.828867   10147 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1118 12:17:57.120918   10147 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1118 12:17:57.120945   10147 ubuntu.go:188] set auth options {CertDir:/home/denys/.minikube CaCertPath:/home/denys/.minikube/certs/ca.pem CaPrivateKeyPath:/home/denys/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/denys/.minikube/machines/server.pem ServerKeyPath:/home/denys/.minikube/machines/server-key.pem ClientKeyPath:/home/denys/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/denys/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/denys/.minikube}
I1118 12:17:57.120966   10147 ubuntu.go:190] setting up certificates
I1118 12:17:57.120983   10147 provision.go:84] configureAuth start
I1118 12:17:57.121122   10147 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1118 12:17:57.201751   10147 provision.go:143] copyHostCerts
I1118 12:17:57.203908   10147 exec_runner.go:151] cp: /home/denys/.minikube/certs/ca.pem --> /home/denys/.minikube/ca.pem (1074 bytes)
I1118 12:17:57.205550   10147 exec_runner.go:151] cp: /home/denys/.minikube/certs/cert.pem --> /home/denys/.minikube/cert.pem (1119 bytes)
I1118 12:17:57.206959   10147 exec_runner.go:151] cp: /home/denys/.minikube/certs/key.pem --> /home/denys/.minikube/key.pem (1679 bytes)
I1118 12:17:57.208552   10147 provision.go:117] generating server cert: /home/denys/.minikube/machines/server.pem ca-key=/home/denys/.minikube/certs/ca.pem private-key=/home/denys/.minikube/certs/ca-key.pem org=denys.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I1118 12:17:57.398902   10147 provision.go:177] copyRemoteCerts
I1118 12:17:57.398972   10147 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1118 12:17:57.399208   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:17:57.437370   10147 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/denys/.minikube/machines/minikube/id_rsa Username:docker}
I1118 12:17:57.624825   10147 ssh_runner.go:362] scp /home/denys/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I1118 12:17:57.726143   10147 ssh_runner.go:362] scp /home/denys/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I1118 12:17:57.843242   10147 ssh_runner.go:362] scp /home/denys/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I1118 12:17:57.949023   10147 provision.go:87] duration metric: took 828.027601ms to configureAuth
I1118 12:17:57.949040   10147 ubuntu.go:206] setting minikube options for container-runtime
I1118 12:17:57.949388   10147 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1118 12:17:57.949428   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:17:57.986026   10147 main.go:141] libmachine: Using SSH client type: native
I1118 12:17:57.986329   10147 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 12:17:57.986339   10147 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1118 12:17:58.249695   10147 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1118 12:17:58.249714   10147 ubuntu.go:71] root file system type: overlay
I1118 12:17:58.249906   10147 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I1118 12:17:58.251723   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:17:58.339820   10147 main.go:141] libmachine: Using SSH client type: native
I1118 12:17:58.340115   10147 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 12:17:58.340191   10147 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 \
	-H fd:// --containerd=/run/containerd/containerd.sock \
	-H unix:///var/run/docker.sock \
	--default-ulimit=nofile=1048576:1048576 \
	--tlsverify \
	--tlscacert /etc/docker/ca.pem \
	--tlscert /etc/docker/server.pem \
	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1118 12:17:58.654947   10147 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
After=network-online.target nss-lookup.target docker.socket firewalld.service containerd.service time-set.target
Wants=network-online.target containerd.service
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=always



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process
OOMScoreAdjust=-500

[Install]
WantedBy=multi-user.target

I1118 12:17:58.655058   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:17:58.703424   10147 main.go:141] libmachine: Using SSH client type: native
I1118 12:17:58.703629   10147 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x840140] 0x842e40 <nil>  [] 0s} 127.0.0.1 32768 <nil> <nil>}
I1118 12:17:58.703643   10147 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1118 12:18:02.355119   10147 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2025-09-03 20:55:49.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-11-18 12:17:58.645988174 +0000
@@ -9,23 +9,34 @@
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
 Restart=always
 
+
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 	-H fd:// --containerd=/run/containerd/containerd.sock 	-H unix:///var/run/docker.sock 	--default-ulimit=nofile=1048576:1048576 	--tlsverify 	--tlscacert /etc/docker/ca.pem 	--tlscert /etc/docker/server.pem 	--tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
+
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I1118 12:18:02.359943   10147 machine.go:96] duration metric: took 9.476254469s to provisionDockerMachine
I1118 12:18:02.359964   10147 client.go:171] duration metric: took 22.261421847s to LocalClient.Create
I1118 12:18:02.360816   10147 start.go:167] duration metric: took 22.262307364s to libmachine.API.Create "minikube"
I1118 12:18:02.360832   10147 start.go:293] postStartSetup for "minikube" (driver="docker")
I1118 12:18:02.360848   10147 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1118 12:18:02.360938   10147 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1118 12:18:02.360987   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:18:02.423054   10147 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/denys/.minikube/machines/minikube/id_rsa Username:docker}
I1118 12:18:02.595259   10147 ssh_runner.go:195] Run: cat /etc/os-release
I1118 12:18:02.626678   10147 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1118 12:18:02.626975   10147 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1118 12:18:02.626990   10147 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1118 12:18:02.627013   10147 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I1118 12:18:02.627031   10147 filesync.go:126] Scanning /home/denys/.minikube/addons for local assets ...
I1118 12:18:02.628980   10147 filesync.go:126] Scanning /home/denys/.minikube/files for local assets ...
I1118 12:18:02.630768   10147 start.go:296] duration metric: took 269.916554ms for postStartSetup
I1118 12:18:02.632896   10147 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1118 12:18:02.724571   10147 profile.go:143] Saving config to /home/denys/.minikube/profiles/minikube/config.json ...
I1118 12:18:02.729490   10147 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1118 12:18:02.729803   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:18:02.810455   10147 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/denys/.minikube/machines/minikube/id_rsa Username:docker}
I1118 12:18:03.023171   10147 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1118 12:18:03.068512   10147 start.go:128] duration metric: took 22.979644445s to createHost
I1118 12:18:03.068539   10147 start.go:83] releasing machines lock for "minikube", held for 22.981050649s
I1118 12:18:03.069631   10147 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1118 12:18:03.171524   10147 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1118 12:18:03.171643   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:18:03.175805   10147 ssh_runner.go:195] Run: cat /version.json
I1118 12:18:03.175862   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:18:03.219884   10147 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/denys/.minikube/machines/minikube/id_rsa Username:docker}
I1118 12:18:03.234666   10147 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/denys/.minikube/machines/minikube/id_rsa Username:docker}
I1118 12:18:03.729869   10147 ssh_runner.go:195] Run: systemctl --version
I1118 12:18:03.755142   10147 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1118 12:18:03.797423   10147 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1118 12:18:03.932165   10147 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1118 12:18:03.932542   10147 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1118 12:18:04.025141   10147 cni.go:262] disabled [/etc/cni/net.d/87-podman-bridge.conflist, /etc/cni/net.d/100-crio-bridge.conf] bridge cni config(s)
I1118 12:18:04.025158   10147 start.go:495] detecting cgroup driver to use...
I1118 12:18:04.025187   10147 detect.go:190] detected "systemd" cgroup driver on host os
I1118 12:18:04.025822   10147 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1118 12:18:04.084711   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10.1"|' /etc/containerd/config.toml"
I1118 12:18:04.121117   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1118 12:18:04.142669   10147 containerd.go:146] configuring containerd to use "systemd" as cgroup driver...
I1118 12:18:04.142715   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1118 12:18:04.164833   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1118 12:18:04.192530   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1118 12:18:04.223016   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1118 12:18:04.267977   10147 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1118 12:18:04.324883   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1118 12:18:04.377020   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I1118 12:18:04.436583   10147 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I1118 12:18:04.482920   10147 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1118 12:18:04.541812   10147 crio.go:166] couldn't verify netfilter by "sudo sysctl net.bridge.bridge-nf-call-iptables" which might be okay. error: sudo sysctl net.bridge.bridge-nf-call-iptables: Process exited with status 255
stdout:

stderr:
sysctl: cannot stat /proc/sys/net/bridge/bridge-nf-call-iptables: No such file or directory
I1118 12:18:04.541866   10147 ssh_runner.go:195] Run: sudo modprobe br_netfilter
I1118 12:18:04.580592   10147 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1118 12:18:04.645029   10147 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 12:18:04.855466   10147 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1118 12:18:05.215714   10147 start.go:495] detecting cgroup driver to use...
I1118 12:18:05.215927   10147 detect.go:190] detected "systemd" cgroup driver on host os
I1118 12:18:05.215967   10147 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1118 12:18:05.251936   10147 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1118 12:18:05.274778   10147 ssh_runner.go:195] Run: sudo systemctl stop -f containerd
I1118 12:18:05.353338   10147 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service containerd
I1118 12:18:05.403882   10147 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1118 12:18:05.472295   10147 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1118 12:18:05.557189   10147 ssh_runner.go:195] Run: which cri-dockerd
I1118 12:18:05.576942   10147 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1118 12:18:05.628786   10147 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (192 bytes)
I1118 12:18:05.766182   10147 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1118 12:18:06.099711   10147 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1118 12:18:06.263684   10147 docker.go:575] configuring docker to use "systemd" as cgroup driver...
I1118 12:18:06.264493   10147 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (129 bytes)
I1118 12:18:06.360767   10147 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I1118 12:18:06.389601   10147 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 12:18:06.571984   10147 ssh_runner.go:195] Run: sudo systemctl restart docker
I1118 12:18:09.656130   10147 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.084125218s)
I1118 12:18:09.656182   10147 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service docker
I1118 12:18:09.687247   10147 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I1118 12:18:09.735103   10147 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1118 12:18:09.791299   10147 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1118 12:18:09.920450   10147 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1118 12:18:10.111714   10147 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 12:18:10.242835   10147 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1118 12:18:10.281067   10147 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I1118 12:18:10.309739   10147 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 12:18:10.452414   10147 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I1118 12:18:10.668911   10147 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I1118 12:18:10.718631   10147 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1118 12:18:10.719465   10147 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1118 12:18:10.739089   10147 start.go:563] Will wait 60s for crictl version
I1118 12:18:10.739170   10147 ssh_runner.go:195] Run: which crictl
I1118 12:18:10.758120   10147 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1118 12:18:10.933685   10147 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.4.0
RuntimeApiVersion:  v1
I1118 12:18:10.933790   10147 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1118 12:18:11.075954   10147 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1118 12:18:11.141552   10147 out.go:252] ðŸ³  Preparing Kubernetes v1.34.0 on Docker 28.4.0 ...
I1118 12:18:11.141718   10147 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1118 12:18:11.179940   10147 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I1118 12:18:11.194025   10147 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1118 12:18:11.226073   10147 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I1118 12:18:11.226464   10147 preload.go:131] Checking if preload exists for k8s version v1.34.0 and runtime docker
I1118 12:18:11.226533   10147 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1118 12:18:11.276079   10147 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1118 12:18:11.276089   10147 docker.go:621] Images already preloaded, skipping extraction
I1118 12:18:11.276144   10147 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1118 12:18:11.321857   10147 docker.go:691] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.34.0
registry.k8s.io/kube-proxy:v1.34.0
registry.k8s.io/kube-scheduler:v1.34.0
registry.k8s.io/kube-controller-manager:v1.34.0
registry.k8s.io/etcd:3.6.4-0
registry.k8s.io/pause:3.10.1
registry.k8s.io/coredns/coredns:v1.12.1
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1118 12:18:11.321871   10147 cache_images.go:85] Images are preloaded, skipping loading
I1118 12:18:11.321878   10147 kubeadm.go:926] updating node { 192.168.49.2 8443 v1.34.0 docker true true} ...
I1118 12:18:11.322083   10147 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.34.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I1118 12:18:11.322131   10147 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1118 12:18:11.442968   10147 cni.go:84] Creating CNI manager for ""
I1118 12:18:11.442989   10147 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1118 12:18:11.443002   10147 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I1118 12:18:11.443018   10147 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.34.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1118 12:18:11.443121   10147 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
kubernetesVersion: v1.34.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1118 12:18:11.443368   10147 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.34.0
I1118 12:18:11.465349   10147 binaries.go:44] Found k8s binaries, skipping transfer
I1118 12:18:11.465412   10147 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1118 12:18:11.483360   10147 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I1118 12:18:11.540257   10147 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1118 12:18:11.592941   10147 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2208 bytes)
I1118 12:18:11.637228   10147 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I1118 12:18:11.650144   10147 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1118 12:18:11.684310   10147 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 12:18:11.809544   10147 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1118 12:18:11.843901   10147 certs.go:68] Setting up /home/denys/.minikube/profiles/minikube for IP: 192.168.49.2
I1118 12:18:11.843913   10147 certs.go:194] generating shared ca certs ...
I1118 12:18:11.843940   10147 certs.go:226] acquiring lock for ca certs: {Name:mk32d9055d5eeb1d9f2edc76761fea762825b973 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:11.844844   10147 certs.go:240] generating "minikubeCA" ca cert: /home/denys/.minikube/ca.key
I1118 12:18:12.009084   10147 crypto.go:156] Writing cert to /home/denys/.minikube/ca.crt ...
I1118 12:18:12.009099   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/ca.crt: {Name:mkea15c7eef291555d87cf214d14ab4887e0c707 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.010584   10147 crypto.go:164] Writing key to /home/denys/.minikube/ca.key ...
I1118 12:18:12.010602   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/ca.key: {Name:mk359a7a5ed7cd43efe09b91b5e8ba648fb53f99 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.010713   10147 certs.go:240] generating "proxyClientCA" ca cert: /home/denys/.minikube/proxy-client-ca.key
I1118 12:18:12.510195   10147 crypto.go:156] Writing cert to /home/denys/.minikube/proxy-client-ca.crt ...
I1118 12:18:12.510211   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/proxy-client-ca.crt: {Name:mk33b0e5deddf4a664f4dec03f926c6444453f8e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.510460   10147 crypto.go:164] Writing key to /home/denys/.minikube/proxy-client-ca.key ...
I1118 12:18:12.510468   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/proxy-client-ca.key: {Name:mkcef8f32443fc854bce33da1c4d819af7672774 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.510566   10147 certs.go:256] generating profile certs ...
I1118 12:18:12.510647   10147 certs.go:363] generating signed profile cert for "minikube-user": /home/denys/.minikube/profiles/minikube/client.key
I1118 12:18:12.510656   10147 crypto.go:68] Generating cert /home/denys/.minikube/profiles/minikube/client.crt with IP's: []
I1118 12:18:12.692793   10147 crypto.go:156] Writing cert to /home/denys/.minikube/profiles/minikube/client.crt ...
I1118 12:18:12.692809   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/profiles/minikube/client.crt: {Name:mk83fb08e641377f300245877261c4c4d010dda9 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.693565   10147 crypto.go:164] Writing key to /home/denys/.minikube/profiles/minikube/client.key ...
I1118 12:18:12.693578   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/profiles/minikube/client.key: {Name:mkd22f11d03f8b01ae18a71fc04d3e5d3d0d4a1c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.694152   10147 certs.go:363] generating signed profile cert for "minikube": /home/denys/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I1118 12:18:12.694364   10147 crypto.go:68] Generating cert /home/denys/.minikube/profiles/minikube/apiserver.crt.7fb57e3c with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 192.168.49.2]
I1118 12:18:12.902700   10147 crypto.go:156] Writing cert to /home/denys/.minikube/profiles/minikube/apiserver.crt.7fb57e3c ...
I1118 12:18:12.902716   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/profiles/minikube/apiserver.crt.7fb57e3c: {Name:mkf8d763e51bd329b1597eefed0676bda83c0270 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.903508   10147 crypto.go:164] Writing key to /home/denys/.minikube/profiles/minikube/apiserver.key.7fb57e3c ...
I1118 12:18:12.903526   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/profiles/minikube/apiserver.key.7fb57e3c: {Name:mkfc110b9aa5135f898500fd9b075293dabb8ac0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.903700   10147 certs.go:381] copying /home/denys/.minikube/profiles/minikube/apiserver.crt.7fb57e3c -> /home/denys/.minikube/profiles/minikube/apiserver.crt
I1118 12:18:12.903793   10147 certs.go:385] copying /home/denys/.minikube/profiles/minikube/apiserver.key.7fb57e3c -> /home/denys/.minikube/profiles/minikube/apiserver.key
I1118 12:18:12.903834   10147 certs.go:363] generating signed profile cert for "aggregator": /home/denys/.minikube/profiles/minikube/proxy-client.key
I1118 12:18:12.903848   10147 crypto.go:68] Generating cert /home/denys/.minikube/profiles/minikube/proxy-client.crt with IP's: []
I1118 12:18:12.959680   10147 crypto.go:156] Writing cert to /home/denys/.minikube/profiles/minikube/proxy-client.crt ...
I1118 12:18:12.959698   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/profiles/minikube/proxy-client.crt: {Name:mk99dc783579fdb451fe1cea429febcae8a348ad Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.960059   10147 crypto.go:164] Writing key to /home/denys/.minikube/profiles/minikube/proxy-client.key ...
I1118 12:18:12.960068   10147 lock.go:35] WriteFile acquiring /home/denys/.minikube/profiles/minikube/proxy-client.key: {Name:mk62f9221c2b37c466f42661730bf029d591d04c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:12.960374   10147 certs.go:484] found cert: /home/denys/.minikube/certs/ca-key.pem (1679 bytes)
I1118 12:18:12.962657   10147 certs.go:484] found cert: /home/denys/.minikube/certs/ca.pem (1074 bytes)
I1118 12:18:12.963844   10147 certs.go:484] found cert: /home/denys/.minikube/certs/cert.pem (1119 bytes)
I1118 12:18:12.964660   10147 certs.go:484] found cert: /home/denys/.minikube/certs/key.pem (1679 bytes)
I1118 12:18:12.966298   10147 ssh_runner.go:362] scp /home/denys/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1118 12:18:13.014433   10147 ssh_runner.go:362] scp /home/denys/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1118 12:18:13.066492   10147 ssh_runner.go:362] scp /home/denys/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1118 12:18:13.128349   10147 ssh_runner.go:362] scp /home/denys/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I1118 12:18:13.202756   10147 ssh_runner.go:362] scp /home/denys/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I1118 12:18:13.264878   10147 ssh_runner.go:362] scp /home/denys/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I1118 12:18:13.312725   10147 ssh_runner.go:362] scp /home/denys/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1118 12:18:13.376059   10147 ssh_runner.go:362] scp /home/denys/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1118 12:18:13.437846   10147 ssh_runner.go:362] scp /home/denys/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1118 12:18:13.500265   10147 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1118 12:18:13.538662   10147 ssh_runner.go:195] Run: openssl version
I1118 12:18:13.558699   10147 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1118 12:18:13.586394   10147 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1118 12:18:13.599074   10147 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Nov 18 12:18 /usr/share/ca-certificates/minikubeCA.pem
I1118 12:18:13.599126   10147 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1118 12:18:13.616046   10147 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1118 12:18:13.639691   10147 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I1118 12:18:13.654544   10147 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I1118 12:18:13.654594   10147 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.48@sha256:7171c97a51623558720f8e5878e4f4637da093e2f2ed589997bedc6c1549b2b1 Memory:3072 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.34.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s MountString: Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false DisableCoreDNSLog:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I1118 12:18:13.654701   10147 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1118 12:18:13.694510   10147 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1118 12:18:13.712461   10147 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1118 12:18:13.731336   10147 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I1118 12:18:13.731390   10147 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1118 12:18:13.751087   10147 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I1118 12:18:13.751098   10147 kubeadm.go:157] found existing configuration files:

I1118 12:18:13.751142   10147 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1118 12:18:13.774687   10147 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I1118 12:18:13.775008   10147 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I1118 12:18:13.790539   10147 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1118 12:18:13.809818   10147 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I1118 12:18:13.809860   10147 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I1118 12:18:13.829073   10147 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1118 12:18:13.854993   10147 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I1118 12:18:13.855047   10147 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1118 12:18:13.886658   10147 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1118 12:18:13.918412   10147 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I1118 12:18:13.918477   10147 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1118 12:18:13.975131   10147 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.34.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I1118 12:18:14.215817   10147 kubeadm.go:310] 	[WARNING SystemVerification]: failed to parse kernel config: unable to load kernel module: "configs", output: "modprobe: FATAL: Module configs not found in directory /lib/modules/6.14.0-35-generic\n", err: exit status 1
I1118 12:18:14.390430   10147 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I1118 12:18:34.314636   10147 kubeadm.go:310] [init] Using Kubernetes version: v1.34.0
I1118 12:18:34.314698   10147 kubeadm.go:310] [preflight] Running pre-flight checks
I1118 12:18:34.314924   10147 kubeadm.go:310] [preflight] The system verification failed. Printing the output from the verification:
I1118 12:18:34.315005   10147 kubeadm.go:310] [0;37mKERNEL_VERSION[0m: [0;32m6.14.0-35-generic[0m
I1118 12:18:34.315046   10147 kubeadm.go:310] [0;37mOS[0m: [0;32mLinux[0m
I1118 12:18:34.315583   10147 kubeadm.go:310] [0;37mCGROUPS_CPU[0m: [0;32menabled[0m
I1118 12:18:34.316530   10147 kubeadm.go:310] [0;37mCGROUPS_CPUSET[0m: [0;32menabled[0m
I1118 12:18:34.318295   10147 kubeadm.go:310] [0;37mCGROUPS_DEVICES[0m: [0;32menabled[0m
I1118 12:18:34.318474   10147 kubeadm.go:310] [0;37mCGROUPS_FREEZER[0m: [0;32menabled[0m
I1118 12:18:34.318685   10147 kubeadm.go:310] [0;37mCGROUPS_MEMORY[0m: [0;32menabled[0m
I1118 12:18:34.318878   10147 kubeadm.go:310] [0;37mCGROUPS_PIDS[0m: [0;32menabled[0m
I1118 12:18:34.319062   10147 kubeadm.go:310] [0;37mCGROUPS_HUGETLB[0m: [0;32menabled[0m
I1118 12:18:34.319255   10147 kubeadm.go:310] [0;37mCGROUPS_IO[0m: [0;32menabled[0m
I1118 12:18:34.319439   10147 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I1118 12:18:34.319627   10147 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I1118 12:18:34.319695   10147 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I1118 12:18:34.319741   10147 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I1118 12:18:34.323760   10147 out.go:252]     â–ª Generating certificates and keys ...
I1118 12:18:34.324085   10147 kubeadm.go:310] [certs] Using existing ca certificate authority
I1118 12:18:34.324446   10147 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I1118 12:18:34.324505   10147 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I1118 12:18:34.324592   10147 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I1118 12:18:34.324638   10147 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I1118 12:18:34.324676   10147 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I1118 12:18:34.325030   10147 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I1118 12:18:34.325122   10147 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1118 12:18:34.325161   10147 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I1118 12:18:34.325259   10147 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I1118 12:18:34.325307   10147 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I1118 12:18:34.325352   10147 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I1118 12:18:34.325385   10147 kubeadm.go:310] [certs] Generating "sa" key and public key
I1118 12:18:34.325426   10147 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I1118 12:18:34.325463   10147 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I1118 12:18:34.325504   10147 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I1118 12:18:34.325542   10147 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I1118 12:18:34.325591   10147 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I1118 12:18:34.325631   10147 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I1118 12:18:34.325712   10147 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I1118 12:18:34.325895   10147 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I1118 12:18:34.336629   10147 out.go:252]     â–ª Booting up control plane ...
I1118 12:18:34.336832   10147 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I1118 12:18:34.336895   10147 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I1118 12:18:34.336948   10147 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I1118 12:18:34.337028   10147 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I1118 12:18:34.337099   10147 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
I1118 12:18:34.337179   10147 kubeadm.go:310] [patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
I1118 12:18:34.337243   10147 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I1118 12:18:34.337273   10147 kubeadm.go:310] [kubelet-start] Starting the kubelet
I1118 12:18:34.337373   10147 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I1118 12:18:34.337611   10147 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I1118 12:18:34.337683   10147 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 1.510000328s
I1118 12:18:34.339179   10147 kubeadm.go:310] [control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
I1118 12:18:34.339290   10147 kubeadm.go:310] [control-plane-check] Checking kube-apiserver at https://192.168.49.2:8443/livez
I1118 12:18:34.339401   10147 kubeadm.go:310] [control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
I1118 12:18:34.339519   10147 kubeadm.go:310] [control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
I1118 12:18:34.339628   10147 kubeadm.go:310] [control-plane-check] kube-controller-manager is healthy after 4.553540358s
I1118 12:18:34.339726   10147 kubeadm.go:310] [control-plane-check] kube-scheduler is healthy after 8.925227945s
I1118 12:18:34.339829   10147 kubeadm.go:310] [control-plane-check] kube-apiserver is healthy after 12.508208944s
I1118 12:18:34.339951   10147 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I1118 12:18:34.340318   10147 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I1118 12:18:34.340881   10147 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I1118 12:18:34.341958   10147 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I1118 12:18:34.342160   10147 kubeadm.go:310] [bootstrap-token] Using token: 53hchd.abeioefk9cti2jzv
I1118 12:18:34.346342   10147 out.go:252]     â–ª Configuring RBAC rules ...
I1118 12:18:34.346500   10147 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I1118 12:18:34.346563   10147 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I1118 12:18:34.346667   10147 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I1118 12:18:34.346860   10147 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I1118 12:18:34.346964   10147 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I1118 12:18:34.347123   10147 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I1118 12:18:34.347332   10147 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I1118 12:18:34.347365   10147 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I1118 12:18:34.347399   10147 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I1118 12:18:34.347401   10147 kubeadm.go:310] 
I1118 12:18:34.347446   10147 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I1118 12:18:34.347448   10147 kubeadm.go:310] 
I1118 12:18:34.347505   10147 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I1118 12:18:34.347507   10147 kubeadm.go:310] 
I1118 12:18:34.347527   10147 kubeadm.go:310]   mkdir -p $HOME/.kube
I1118 12:18:34.347570   10147 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I1118 12:18:34.347608   10147 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I1118 12:18:34.347610   10147 kubeadm.go:310] 
I1118 12:18:34.347649   10147 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I1118 12:18:34.347651   10147 kubeadm.go:310] 
I1118 12:18:34.347689   10147 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I1118 12:18:34.347691   10147 kubeadm.go:310] 
I1118 12:18:34.347729   10147 kubeadm.go:310] You should now deploy a pod network to the cluster.
I1118 12:18:34.347880   10147 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I1118 12:18:34.347932   10147 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I1118 12:18:34.347934   10147 kubeadm.go:310] 
I1118 12:18:34.347997   10147 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I1118 12:18:34.348146   10147 kubeadm.go:310] and service account keys on each node and then running the following as root:
I1118 12:18:34.348149   10147 kubeadm.go:310] 
I1118 12:18:34.348531   10147 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 53hchd.abeioefk9cti2jzv \
I1118 12:18:34.348783   10147 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:444915e1f7476466736521f0c834b496dc87c68aa7c2da9749420c9238a19ba5 \
I1118 12:18:34.348799   10147 kubeadm.go:310] 	--control-plane 
I1118 12:18:34.348801   10147 kubeadm.go:310] 
I1118 12:18:34.349000   10147 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I1118 12:18:34.349003   10147 kubeadm.go:310] 
I1118 12:18:34.349171   10147 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 53hchd.abeioefk9cti2jzv \
I1118 12:18:34.349548   10147 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:444915e1f7476466736521f0c834b496dc87c68aa7c2da9749420c9238a19ba5 
I1118 12:18:34.349556   10147 cni.go:84] Creating CNI manager for ""
I1118 12:18:34.349565   10147 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1118 12:18:34.357806   10147 out.go:179] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I1118 12:18:34.374394   10147 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1118 12:18:34.439753   10147 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I1118 12:18:34.539731   10147 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1118 12:18:34.539967   10147 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I1118 12:18:34.540034   10147 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_11_18T12_18_34_0700 minikube.k8s.io/version=v1.37.0 minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3 minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I1118 12:18:35.356353   10147 ops.go:34] apiserver oom_adj: -16
I1118 12:18:35.356400   10147 kubeadm.go:1105] duration metric: took 816.470626ms to wait for elevateKubeSystemPrivileges
I1118 12:18:35.356492   10147 kubeadm.go:394] duration metric: took 21.701902543s to StartCluster
I1118 12:18:35.356512   10147 settings.go:142] acquiring lock: {Name:mked27d6dcf6a5632ad32e517d5a9dfc7ab1ef18 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:35.356900   10147 settings.go:150] Updating kubeconfig:  /home/denys/.kube/config
I1118 12:18:35.360254   10147 lock.go:35] WriteFile acquiring /home/denys/.kube/config: {Name:mk073a049b3c0cae0780e0e613c0e8a4a634df74 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1118 12:18:35.360953   10147 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1118 12:18:35.361585   10147 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.34.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I1118 12:18:35.362789   10147 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.34.0
I1118 12:18:35.362817   10147 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubetail:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I1118 12:18:35.362877   10147 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1118 12:18:35.362890   10147 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I1118 12:18:35.362912   10147 host.go:66] Checking if "minikube" exists ...
I1118 12:18:35.363936   10147 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 12:18:35.365461   10147 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1118 12:18:35.365476   10147 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I1118 12:18:35.365669   10147 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 12:18:35.370581   10147 out.go:179] ðŸ”Ž  Verifying Kubernetes components...
I1118 12:18:35.375461   10147 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1118 12:18:35.728715   10147 out.go:179]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I1118 12:18:35.737165   10147 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1118 12:18:35.737176   10147 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1118 12:18:35.737230   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:18:35.857725   10147 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/denys/.minikube/machines/minikube/id_rsa Username:docker}
I1118 12:18:35.859050   10147 addons.go:238] Setting addon default-storageclass=true in "minikube"
I1118 12:18:35.859086   10147 host.go:66] Checking if "minikube" exists ...
I1118 12:18:35.859638   10147 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1118 12:18:35.914814   10147 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml | sed -e '/^        forward . \/etc\/resolv.conf.*/i \        hosts {\n           192.168.49.1 host.minikube.internal\n           fallthrough\n        }' -e '/^        errors *$/i \        log' | sudo /var/lib/minikube/binaries/v1.34.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig replace -f -"
I1118 12:18:35.926875   10147 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I1118 12:18:35.926899   10147 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1118 12:18:35.927152   10147 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1118 12:18:35.968464   10147 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32768 SSHKeyPath:/home/denys/.minikube/machines/minikube/id_rsa Username:docker}
I1118 12:18:36.069780   10147 ssh_runner.go:195] Run: sudo systemctl start kubelet
I1118 12:18:36.351167   10147 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1118 12:18:36.410692   10147 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1118 12:18:36.888432   10147 start.go:976] {"host.minikube.internal": 192.168.49.1} host record injected into CoreDNS's ConfigMap
I1118 12:18:36.892008   10147 api_server.go:52] waiting for apiserver process to appear ...
I1118 12:18:36.892080   10147 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1118 12:18:37.412616   10147 kapi.go:214] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1118 12:18:37.589735   10147 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (1.23854371s)
I1118 12:18:37.589783   10147 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.34.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (1.179080382s)
I1118 12:18:37.590003   10147 api_server.go:72] duration metric: took 2.228391384s to wait for apiserver process to appear ...
I1118 12:18:37.590009   10147 api_server.go:88] waiting for apiserver healthz status ...
I1118 12:18:37.590023   10147 api_server.go:253] Checking apiserver healthz at https://192.168.49.2:8443/healthz ...
I1118 12:18:37.651805   10147 api_server.go:279] https://192.168.49.2:8443/healthz returned 200:
ok
I1118 12:18:37.656804   10147 api_server.go:141] control plane version: v1.34.0
I1118 12:18:37.656822   10147 api_server.go:131] duration metric: took 66.808893ms to wait for apiserver health ...
I1118 12:18:37.656831   10147 system_pods.go:43] waiting for kube-system pods to appear ...
I1118 12:18:37.674567   10147 out.go:179] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I1118 12:18:37.680940   10147 addons.go:514] duration metric: took 2.31809991s for enable addons: enabled=[storage-provisioner default-storageclass]
I1118 12:18:37.701508   10147 system_pods.go:59] 5 kube-system pods found
I1118 12:18:37.701682   10147 system_pods.go:61] "etcd-minikube" [418d2db9-3d11-4781-bcf8-99072d68c997] Running
I1118 12:18:37.701691   10147 system_pods.go:61] "kube-apiserver-minikube" [6409cbc9-eb84-46d3-8ecd-c0a36a7add03] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I1118 12:18:37.701697   10147 system_pods.go:61] "kube-controller-manager-minikube" [34f677f6-77c9-435a-9a1f-6287ba84bc3e] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I1118 12:18:37.701701   10147 system_pods.go:61] "kube-scheduler-minikube" [c1cea63a-9ae3-4326-abc1-dced3c7e16e0] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I1118 12:18:37.701718   10147 system_pods.go:61] "storage-provisioner" [2d8ac0db-3391-4a41-a2a4-04b44129e3a4] Pending: PodScheduled:Unschedulable (0/1 nodes are available: 1 node(s) had untolerated taint {node.kubernetes.io/not-ready: }. no new claims to deallocate, preemption: 0/1 nodes are available: 1 Preemption is not helpful for scheduling.)
I1118 12:18:37.701724   10147 system_pods.go:74] duration metric: took 44.889098ms to wait for pod list to return data ...
I1118 12:18:37.701738   10147 kubeadm.go:578] duration metric: took 2.340123147s to wait for: map[apiserver:true system_pods:true]
I1118 12:18:37.701750   10147 node_conditions.go:102] verifying NodePressure condition ...
I1118 12:18:37.706830   10147 node_conditions.go:122] node storage ephemeral capacity is 35809824Ki
I1118 12:18:37.706856   10147 node_conditions.go:123] node cpu capacity is 2
I1118 12:18:37.706869   10147 node_conditions.go:105] duration metric: took 5.115816ms to run NodePressure ...
I1118 12:18:37.706879   10147 start.go:241] waiting for startup goroutines ...
I1118 12:18:37.706884   10147 start.go:246] waiting for cluster config update ...
I1118 12:18:37.706893   10147 start.go:255] writing updated cluster config ...
I1118 12:18:37.715010   10147 ssh_runner.go:195] Run: rm -f paused
I1118 12:18:37.740169   10147 out.go:179] ðŸ’¡  kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I1118 12:18:37.751028   10147 out.go:179] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Nov 18 12:18:02 minikube dockerd[639]: time="2025-11-18T12:18:02.333366133Z" level=info msg="Completed buildkit initialization"
Nov 18 12:18:02 minikube dockerd[639]: time="2025-11-18T12:18:02.341998887Z" level=info msg="Daemon has completed initialization"
Nov 18 12:18:02 minikube dockerd[639]: time="2025-11-18T12:18:02.343432619Z" level=info msg="API listen on /run/docker.sock"
Nov 18 12:18:02 minikube dockerd[639]: time="2025-11-18T12:18:02.343516254Z" level=info msg="API listen on [::]:2376"
Nov 18 12:18:02 minikube systemd[1]: Started Docker Application Container Engine.
Nov 18 12:18:02 minikube dockerd[639]: time="2025-11-18T12:18:02.345141474Z" level=info msg="API listen on /var/run/docker.sock"
Nov 18 12:18:04 minikube dockerd[639]: time="2025-11-18T12:18:04.925325775Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Nov 18 12:18:04 minikube dockerd[639]: time="2025-11-18T12:18:04.925773647Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Nov 18 12:18:04 minikube dockerd[639]: time="2025-11-18T12:18:04.926164010Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Nov 18 12:18:04 minikube dockerd[639]: time="2025-11-18T12:18:04.926215774Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Nov 18 12:18:05 minikube dockerd[639]: time="2025-11-18T12:18:05.320338063Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=plugins.moby
Nov 18 12:18:05 minikube dockerd[639]: time="2025-11-18T12:18:05.320390440Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=plugins.moby
Nov 18 12:18:05 minikube dockerd[639]: time="2025-11-18T12:18:05.320811467Z" level=error msg="Failed to get event" error="rpc error: code = Unavailable desc = error reading from server: EOF" module=libcontainerd namespace=moby
Nov 18 12:18:05 minikube dockerd[639]: time="2025-11-18T12:18:05.320835345Z" level=info msg="Waiting for containerd to be ready to restart event processing" module=libcontainerd namespace=moby
Nov 18 12:18:06 minikube systemd[1]: Stopping Docker Application Container Engine...
Nov 18 12:18:06 minikube dockerd[639]: time="2025-11-18T12:18:06.592375767Z" level=info msg="Processing signal 'terminated'"
Nov 18 12:18:06 minikube dockerd[639]: time="2025-11-18T12:18:06.595502466Z" level=warning msg="Error while testing if containerd API is ready" error="Canceled: grpc: the client connection is closing"
Nov 18 12:18:06 minikube dockerd[639]: time="2025-11-18T12:18:06.597191012Z" level=info msg="Daemon shutdown complete"
Nov 18 12:18:06 minikube systemd[1]: docker.service: Deactivated successfully.
Nov 18 12:18:06 minikube systemd[1]: Stopped Docker Application Container Engine.
Nov 18 12:18:06 minikube systemd[1]: docker.service: Consumed 2.324s CPU time.
Nov 18 12:18:06 minikube systemd[1]: Starting Docker Application Container Engine...
Nov 18 12:18:06 minikube dockerd[1064]: time="2025-11-18T12:18:06.880475917Z" level=info msg="Starting up"
Nov 18 12:18:06 minikube dockerd[1064]: time="2025-11-18T12:18:06.882905514Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Nov 18 12:18:06 minikube dockerd[1064]: time="2025-11-18T12:18:06.883121868Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/etc/cdi
Nov 18 12:18:06 minikube dockerd[1064]: time="2025-11-18T12:18:06.883254752Z" level=info msg="CDI directory does not exist, skipping: failed to monitor for changes: no such file or directory" dir=/var/run/cdi
Nov 18 12:18:06 minikube dockerd[1064]: time="2025-11-18T12:18:06.915481702Z" level=info msg="Creating a containerd client" address=/run/containerd/containerd.sock timeout=1m0s
Nov 18 12:18:06 minikube dockerd[1064]: time="2025-11-18T12:18:06.930101068Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Nov 18 12:18:07 minikube dockerd[1064]: time="2025-11-18T12:18:07.011616663Z" level=info msg="Loading containers: start."
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.301670955Z" level=warning msg="Error (Unable to complete atomic operation, key modified) deleting object [endpoint_count f53aa82e6c2a829e8a740a5c433756569680a240db3728d24aee5dd0749ba487], retrying...."
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.580497788Z" level=info msg="Loading containers: done."
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.625941997Z" level=info msg="Docker daemon" commit=249d679 containerd-snapshotter=false storage-driver=overlay2 version=28.4.0
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.626125991Z" level=info msg="Initializing buildkit"
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.640617964Z" level=info msg="Completed buildkit initialization"
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.646349676Z" level=info msg="Daemon has completed initialization"
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.647401529Z" level=info msg="API listen on /run/docker.sock"
Nov 18 12:18:09 minikube systemd[1]: Started Docker Application Container Engine.
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.648813389Z" level=info msg="API listen on [::]:2376"
Nov 18 12:18:09 minikube dockerd[1064]: time="2025-11-18T12:18:09.649269456Z" level=info msg="API listen on /var/run/docker.sock"
Nov 18 12:18:10 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Start docker client with request timeout 0s"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Hairpin mode is set to hairpin-veth"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Loaded network plugin cni"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Docker cri networking managed by network plugin cni"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Setting cgroupDriver systemd"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Nov 18 12:18:10 minikube cri-dockerd[1360]: time="2025-11-18T12:18:10Z" level=info msg="Start cri-dockerd grpc backend"
Nov 18 12:18:10 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Nov 18 12:18:21 minikube cri-dockerd[1360]: time="2025-11-18T12:18:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/da7463261512d5befa7ee88c7d5a15a8ac0898a3e59cc10b9c1d3df38e875fa3/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Nov 18 12:18:21 minikube cri-dockerd[1360]: time="2025-11-18T12:18:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5c5efaf70d499e40c827944bc02e905997de731927f6e5d249469032ea04f3d3/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Nov 18 12:18:21 minikube cri-dockerd[1360]: time="2025-11-18T12:18:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/0e7b9b7c3801a37a5af5216433907e8b5078acbf7c420d0aaf5cd2d9274de173/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Nov 18 12:18:21 minikube cri-dockerd[1360]: time="2025-11-18T12:18:21Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b252b3d29177aae573c7b6b87c567ddc876815fb2760813931c4bd63f2f367c3/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Nov 18 12:18:39 minikube cri-dockerd[1360]: time="2025-11-18T12:18:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6f7bcfd98066f1c9f5b88b87921c4ca95f15591b4e46cefbbf9fee824f3dabc8/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Nov 18 12:18:40 minikube cri-dockerd[1360]: time="2025-11-18T12:18:40Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e0c6b22966399a77e735863d6da83a09e7a400404fca58a499c1f4e40f2299dc/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Nov 18 12:18:41 minikube cri-dockerd[1360]: time="2025-11-18T12:18:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/54a80b4ab01d264e34d0f2cccbf3e1f46fe7d14e4221af4c83c249e6983f79dd/resolv.conf as [nameserver 192.168.49.1 search home options edns0 trust-ad ndots:0]"
Nov 18 12:18:44 minikube cri-dockerd[1360]: time="2025-11-18T12:18:44Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Nov 18 12:19:01 minikube dockerd[1064]: time="2025-11-18T12:19:01.840130773Z" level=info msg="ignoring event" container=10a75222f40cd056adbd8235f1d7c697427c6ca102cf4a88013feeb6b4180878 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
93867fa34e984       6e38f40d628db       9 minutes ago       Running             storage-provisioner       1                   6f7bcfd98066f       storage-provisioner
aff6de6ad1788       52546a367cc9e       9 minutes ago       Running             coredns                   0                   54a80b4ab01d2       coredns-66bc5c9577-bcv8b
4b0372da560bf       df0860106674d       9 minutes ago       Running             kube-proxy                0                   e0c6b22966399       kube-proxy-ghqs4
10a75222f40cd       6e38f40d628db       9 minutes ago       Exited              storage-provisioner       0                   6f7bcfd98066f       storage-provisioner
724a2ed0fc917       5f1f5298c888d       10 minutes ago      Running             etcd                      0                   b252b3d29177a       etcd-minikube
744869ea363f6       a0af72f2ec6d6       10 minutes ago      Running             kube-controller-manager   0                   0e7b9b7c3801a       kube-controller-manager-minikube
f8a63359a8b0d       90550c43ad2bc       10 minutes ago      Running             kube-apiserver            0                   5c5efaf70d499       kube-apiserver-minikube
67fdad5f3b4dc       46169d968e920       10 minutes ago      Running             kube-scheduler            0                   da7463261512d       kube-scheduler-minikube


==> coredns [aff6de6ad178] <==
maxprocs: Leaving GOMAXPROCS=2: CPU quota undefined
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 9e2996f8cb67ac53e0259ab1f8d615d07d1beb0bd07e6a1e39769c3bf486a905bb991cc47f8d2f14d0d3a90a87dfc625a0b4c524fed169d8158c40657c0694b1
CoreDNS-1.12.1
linux/amd64, go1.24.1, 707c7c1
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:45401 - 13691 "HINFO IN 3477870104104288862.3272743256458759042. udp 57 false 512" NXDOMAIN qr,rd,ra 132 0.042886521s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.32.3/tools/cache/reflector.go:251: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error
[ERROR] plugin/kubernetes: Unhandled Error


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=65318f4cfff9c12cc87ec9eb8f4cdd57b25047f3
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_11_18T12_18_34_0700
                    minikube.k8s.io/version=v1.37.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 18 Nov 2025 12:18:28 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Tue, 18 Nov 2025 12:28:37 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Tue, 18 Nov 2025 12:28:17 +0000   Tue, 18 Nov 2025 12:18:26 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Tue, 18 Nov 2025 12:28:17 +0000   Tue, 18 Nov 2025 12:18:26 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Tue, 18 Nov 2025 12:28:17 +0000   Tue, 18 Nov 2025 12:18:26 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Tue, 18 Nov 2025 12:28:17 +0000   Tue, 18 Nov 2025 12:28:17 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                2
  ephemeral-storage:  35809824Ki
  hugepages-2Mi:      0
  memory:             7654988Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  35809824Ki
  hugepages-2Mi:      0
  memory:             7654988Ki
  pods:               110
System Info:
  Machine ID:                 114c6d12a4c44a8cbde5af30acc14998
  System UUID:                097de974-fc27-47f5-a719-8098203c56e3
  Boot ID:                    5424f267-e5a4-40c3-97af-1ee4c02b137d
  Kernel Version:             6.14.0-35-generic
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.4.0
  Kubelet Version:            v1.34.0
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-66bc5c9577-bcv8b            100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     9m59s
  kube-system                 etcd-minikube                       100m (5%)     0 (0%)      100Mi (1%)       0 (0%)         10m
  kube-system                 kube-apiserver-minikube             250m (12%)    0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-controller-manager-minikube    200m (10%)    0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 kube-proxy-ghqs4                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         9m59s
  kube-system                 kube-scheduler-minikube             100m (5%)     0 (0%)      0 (0%)           0 (0%)         10m
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         10m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (37%)  0 (0%)
  memory             170Mi (2%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 9m56s              kube-proxy       
  Normal  Starting                 10m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  10m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  10m (x8 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    10m (x8 over 10m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     10m (x7 over 10m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 10m                kubelet          Starting kubelet.
  Normal  NodeAllocatableEnforced  10m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  10m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    10m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     10m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  RegisteredNode           10m                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeNotReady             33s                kubelet          Node minikube status is now: NodeNotReady
  Normal  NodeReady                21s                kubelet          Node minikube status is now: NodeReady


==> dmesg <==
[  +0.000002] vmwgfx 0000:00:02.0: [drm] *ERROR* This configuration is likely broken.
[  +0.000001] vmwgfx 0000:00:02.0: [drm] *ERROR* Please switch to a supported graphics device to avoid problems.
[  +0.365206] kauditd_printk_skb: 81 callbacks suppressed
[  +5.180744] kauditd_printk_skb: 43 callbacks suppressed
[  +5.590998] kauditd_printk_skb: 4 callbacks suppressed
[ +12.970806] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 1112696249 wd_nsec: 1112696050
[Nov18 11:21] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 2833511010 wd_nsec: 2833510374
[Nov18 11:29] watchdog: BUG: soft lockup - CPU#0 stuck for 31s! [swapper/0:0]
[  +0.000033] Modules linked in: snd_seq_dummy snd_hrtimer qrtr snd_intel8x0 snd_ac97_codec ac97_bus snd_pcm snd_seq_midi snd_seq_midi_event snd_rawmidi intel_rapl_msr intel_rapl_common snd_seq intel_uncore_frequency_common intel_pmc_core pmt_telemetry pmt_class intel_vsec snd_seq_device snd_timer polyval_clmulni polyval_generic ghash_clmulni_intel sha256_ssse3 sha1_ssse3 aesni_intel crypto_simd cryptd vmwgfx rapl snd drm_ttm_helper soundcore i2c_piix4 i2c_smbus vboxguest ttm joydev input_leds mac_hid serio_raw binfmt_misc sch_fq_codel msr parport_pc ppdev lp parport efi_pstore nfnetlink dmi_sysfs ip_tables x_tables autofs4 hid_generic usbhid vga16fb hid video vgastate psmouse ahci wmi e1000 libahci pata_acpi
[  +0.000085] CPU: 0 UID: 0 PID: 0 Comm: swapper/0 Not tainted 6.14.0-35-generic #35~24.04.1-Ubuntu
[  +0.000003] Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
[  +0.000001] RIP: 0010:pv_native_safe_halt+0xb/0x10
[  +0.000008] Code: 22 d7 31 ff e9 c6 19 01 00 66 0f 1f 44 00 00 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 eb 07 0f 00 2d 19 12 13 00 fb f4 <c3> cc cc cc cc 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 90 83
[  +0.000002] RSP: 0018:ffffffff8ce03df0 EFLAGS: 00010246
[  +0.000002] RAX: 0000000000000000 RBX: 0000000000000000 RCX: 0000000000000000
[  +0.000001] RDX: 0000000000000000 RSI: 0000000000000000 RDI: 0000000000000000
[  +0.000001] RBP: ffffffff8ce03df8 R08: 0000000000000000 R09: 0000000000000000
[  +0.000001] R10: 0000000000000000 R11: 0000000000000000 R12: ffffffff8ce10f00
[  +0.000001] R13: 0000000000000000 R14: 0000000000000000 R15: 000000000008a000
[  +0.000001] FS:  0000000000000000(0000) GS:ffff8f407a800000(0000) knlGS:0000000000000000
[  +0.000002] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
[  +0.000001] CR2: 000000c00051f010 CR3: 000000011d0e2001 CR4: 00000000000706f0
[  +0.000003] Call Trace:
[  +0.000001]  <TASK>
[  +0.000001]  ? default_idle+0x9/0x30
[  +0.000008]  arch_cpu_idle+0x9/0x10
[  +0.000002]  default_idle_call+0x30/0x100
[  +0.000003]  cpuidle_idle_call+0x14f/0x190
[  +0.000005]  do_idle+0x7f/0xe0
[  +0.000002]  cpu_startup_entry+0x29/0x30
[  +0.000001]  rest_init+0xde/0x100
[  +0.000003]  start_kernel+0x3da/0x510
[  +0.000004]  x86_64_start_reservations+0x18/0x30
[  +0.000003]  x86_64_start_kernel+0xbf/0x110
[  +0.000001]  common_startup_64+0x13e/0x141
[  +0.000020]  </TASK>
[  +0.000083] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 34189432601 wd_nsec: 34189424252
[Nov18 11:31] kauditd_printk_skb: 9 callbacks suppressed
[Nov18 11:50] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 4 times, consider switching to WQ_UNBOUND
[ +40.961584] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 5 times, consider switching to WQ_UNBOUND
[Nov18 11:55] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 7 times, consider switching to WQ_UNBOUND
[Nov18 12:06] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 11 times, consider switching to WQ_UNBOUND
[Nov18 12:17] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 19 times, consider switching to WQ_UNBOUND
[Nov18 12:18] workqueue: blk_mq_run_work_fn hogged CPU for >10000us 35 times, consider switching to WQ_UNBOUND
[Nov18 12:28] watchdog: BUG: soft lockup - CPU#0 stuck for 115s! [llvmpipe-0:2702]
[  +0.000009] Modules linked in: xt_mark xt_nfacct ipt_REJECT nf_reject_ipv4 nfnetlink_acct xt_comment br_netfilter xt_nat xt_tcpudp nf_conntrack_netlink veth xt_conntrack xt_MASQUERADE bridge stp llc xfrm_user xfrm_algo xt_set ip_set nft_chain_nat nf_nat nf_conntrack nf_defrag_ipv6 nf_defrag_ipv4 xt_addrtype nft_compat nf_tables overlay snd_seq_dummy snd_hrtimer qrtr snd_intel8x0 snd_ac97_codec ac97_bus snd_pcm snd_seq_midi snd_seq_midi_event snd_rawmidi intel_rapl_msr intel_rapl_common snd_seq intel_uncore_frequency_common intel_pmc_core pmt_telemetry pmt_class intel_vsec snd_seq_device snd_timer polyval_clmulni polyval_generic ghash_clmulni_intel sha256_ssse3 sha1_ssse3 aesni_intel crypto_simd cryptd vmwgfx rapl snd drm_ttm_helper soundcore i2c_piix4 i2c_smbus vboxguest ttm joydev input_leds mac_hid serio_raw binfmt_misc sch_fq_codel msr parport_pc ppdev lp parport efi_pstore nfnetlink dmi_sysfs ip_tables x_tables autofs4 hid_generic usbhid vga16fb hid video vgastate psmouse ahci wmi e1000 libahci pata_acpi
[  +0.000090] CPU: 0 UID: 1000 PID: 2702 Comm: llvmpipe-0 Tainted: G             L     6.14.0-35-generic #35~24.04.1-Ubuntu
[  +0.000003] Tainted: [L]=SOFTLOCKUP
[  +0.000001] Hardware name: innotek GmbH VirtualBox/VirtualBox, BIOS VirtualBox 12/01/2006
[  +0.000023] RIP: 0033:0x79224924fa3c
[  +0.003511] Code: e1 66 0f fe dc 66 0f 6b d4 66 0f fe cb 66 0f 6f eb 66 0f 6b e9 66 0f 63 d5 66 0f d7 d2 66 0f 70 d7 00 66 0f fe c2 66 0f fe e2 <41> 09 d4 66 0f fe da 66 0f fe ca 66 0f 6b c4 66 0f 6b d9 66 0f 63
[  +0.000002] RSP: 002b:0000792233ffdd90 EFLAGS: 00010202
[  +0.000003] RAX: 00000000fff55400 RBX: 0000792233ffde00 RCX: 0000000000515f80
[  +0.000002] RDX: 0000000000000000 RSI: 00000000003c0780 RDI: 0000792233ffde58
[  +0.000001] RBP: 0000792233ffdec0 R08: 0000000000315b80 R09: 00000000000aac00
[  +0.000001] R10: 0000792233ffde08 R11: 0000000000000000 R12: 0000000000000000
[  +0.000001] R13: 00000000000aac00 R14: 0000792233ffde88 R15: 000000000002ab00
[  +0.000001] FS:  0000792233fff6c0 GS:  0000000000000000
[  +0.000705] clocksource: Long readout interval, skipping watchdog check: cs_nsec: 122940361624 wd_nsec: 122940331653
[ +13.009868] systemd-journald[4211]: File /run/log/journal/114c6d12a4c44a8cbde5af30acc14998/system.journal corrupted or uncleanly shut down, renaming and replacing.


==> etcd [724a2ed0fc91] <==
{"level":"warn","ts":"2025-11-18T12:18:26.513948Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40692","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.539200Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40714","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.575170Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40742","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.599485Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40760","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.629943Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40776","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.652853Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40806","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.665028Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40812","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.680831Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40822","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.698711Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40840","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.707231Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40862","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.742144Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40870","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.749060Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40886","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.759781Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40906","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.772885Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40928","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.794212Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40936","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.806019Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40966","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.822270Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40984","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.848119Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:40996","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.856834Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41012","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.873407Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41028","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.889182Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41040","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.904059Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41044","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.929548Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41064","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.938273Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41084","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.948492Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41108","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.965650Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41126","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.980050Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41144","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.988859Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41160","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:26.997252Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41188","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:27.021179Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41208","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:27.039170Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41216","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:27.051880Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41236","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:27.062570Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41250","server-name":"","error":"EOF"}
{"level":"warn","ts":"2025-11-18T12:18:27.177539Z","caller":"embed/config_logging.go:188","msg":"rejected connection on client endpoint","remote-addr":"127.0.0.1:41270","server-name":"","error":"EOF"}
{"level":"info","ts":"2025-11-18T12:18:34.699987Z","caller":"traceutil/trace.go:172","msg":"trace[427595427] transaction","detail":"{read_only:false; response_revision:303; number_of_response:1; }","duration":"113.534686ms","start":"2025-11-18T12:18:34.586428Z","end":"2025-11-18T12:18:34.699963Z","steps":["trace[427595427] 'process raft request'  (duration: 20.833148ms)","trace[427595427] 'marshal mvccpb.KeyValue' {req_type:put; key:/registry/events/default/minikube.18791916bbec5bb2; req_size:635; } (duration: 92.038946ms)"],"step_count":2}
{"level":"warn","ts":"2025-11-18T12:18:35.808697Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"106.734493ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/ttl-after-finished-controller\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-18T12:18:35.809066Z","caller":"traceutil/trace.go:172","msg":"trace[499480440] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/ttl-after-finished-controller; range_end:; response_count:0; response_revision:320; }","duration":"108.705576ms","start":"2025-11-18T12:18:35.700343Z","end":"2025-11-18T12:18:35.809048Z","steps":["trace[499480440] 'range keys from in-memory index tree'  (duration: 106.624973ms)"],"step_count":1}
{"level":"info","ts":"2025-11-18T12:18:36.010157Z","caller":"traceutil/trace.go:172","msg":"trace[479145573] transaction","detail":"{read_only:false; response_revision:322; number_of_response:1; }","duration":"129.259549ms","start":"2025-11-18T12:18:35.880879Z","end":"2025-11-18T12:18:36.010139Z","steps":["trace[479145573] 'process raft request'  (duration: 124.847977ms)"],"step_count":1}
{"level":"info","ts":"2025-11-18T12:19:51.199950Z","caller":"traceutil/trace.go:172","msg":"trace[625385989] transaction","detail":"{read_only:false; response_revision:481; number_of_response:1; }","duration":"180.325498ms","start":"2025-11-18T12:19:51.019607Z","end":"2025-11-18T12:19:51.199933Z","steps":["trace[625385989] 'process raft request'  (duration: 180.201257ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-18T12:19:53.467900Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"178.815662ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/priorityclasses\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-18T12:19:53.471805Z","caller":"traceutil/trace.go:172","msg":"trace[349385756] range","detail":"{range_begin:/registry/priorityclasses; range_end:; response_count:0; response_revision:481; }","duration":"182.717711ms","start":"2025-11-18T12:19:53.289064Z","end":"2025-11-18T12:19:53.471782Z","steps":["trace[349385756] 'range keys from in-memory index tree'  (duration: 178.770452ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-18T12:19:53.472617Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"183.586659ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.49.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-11-18T12:19:53.472683Z","caller":"traceutil/trace.go:172","msg":"trace[252810178] range","detail":"{range_begin:/registry/masterleases/192.168.49.2; range_end:; response_count:1; response_revision:481; }","duration":"183.665454ms","start":"2025-11-18T12:19:53.289009Z","end":"2025-11-18T12:19:53.472675Z","steps":["trace[252810178] 'range keys from in-memory index tree'  (duration: 183.053274ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-18T12:19:53.472913Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"183.185356ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/secrets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-18T12:19:53.472955Z","caller":"traceutil/trace.go:172","msg":"trace[774196423] range","detail":"{range_begin:/registry/secrets; range_end:; response_count:0; response_revision:481; }","duration":"183.229247ms","start":"2025-11-18T12:19:53.289719Z","end":"2025-11-18T12:19:53.472948Z","steps":["trace[774196423] 'range keys from in-memory index tree'  (duration: 183.123154ms)"],"step_count":1}
{"level":"info","ts":"2025-11-18T12:19:54.347928Z","caller":"traceutil/trace.go:172","msg":"trace[1107014801] transaction","detail":"{read_only:false; response_revision:482; number_of_response:1; }","duration":"839.086618ms","start":"2025-11-18T12:19:53.508823Z","end":"2025-11-18T12:19:54.347909Z","steps":["trace[1107014801] 'process raft request'  (duration: 801.983579ms)","trace[1107014801] 'compare'  (duration: 13.92123ms)"],"step_count":2}
{"level":"warn","ts":"2025-11-18T12:19:54.350154Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-11-18T12:19:53.508815Z","time spent":"839.178778ms","remote":"127.0.0.1:59368","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":116,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/masterleases/192.168.49.2\" mod_revision:475 > success:<request_put:<key:\"/registry/masterleases/192.168.49.2\" value_size:65 lease:8128041400405738071 >> failure:<request_range:<key:\"/registry/masterleases/192.168.49.2\" > >"}
{"level":"info","ts":"2025-11-18T12:19:55.085768Z","caller":"traceutil/trace.go:172","msg":"trace[579490255] transaction","detail":"{read_only:false; response_revision:483; number_of_response:1; }","duration":"733.617918ms","start":"2025-11-18T12:19:54.352128Z","end":"2025-11-18T12:19:55.085746Z","steps":["trace[579490255] 'process raft request'  (duration: 712.790078ms)","trace[579490255] 'compare'  (duration: 12.879649ms)"],"step_count":2}
{"level":"warn","ts":"2025-11-18T12:19:55.085898Z","caller":"v3rpc/interceptor.go:202","msg":"request stats","start time":"2025-11-18T12:19:54.352111Z","time spent":"733.730225ms","remote":"127.0.0.1:59620","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":585,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:480 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:512 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2025-11-18T12:28:06.687230Z","caller":"traceutil/trace.go:172","msg":"trace[1254882609] transaction","detail":"{read_only:false; response_revision:774; number_of_response:1; }","duration":"103.442793ms","start":"2025-11-18T12:28:06.583768Z","end":"2025-11-18T12:28:06.687211Z","steps":["trace[1254882609] 'process raft request'  (duration: 96.042312ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-18T12:28:06.727152Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"111.187777ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/statefulsets\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"warn","ts":"2025-11-18T12:28:06.730735Z","caller":"txn/util.go:93","msg":"apply request took too long","took":"114.80516ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/jobs\" limit:1 ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-11-18T12:28:06.755839Z","caller":"traceutil/trace.go:172","msg":"trace[982722817] range","detail":"{range_begin:/registry/jobs; range_end:; response_count:0; response_revision:775; }","duration":"139.919264ms","start":"2025-11-18T12:28:06.615910Z","end":"2025-11-18T12:28:06.755829Z","steps":["trace[982722817] 'agreement among raft nodes before linearized reading'  (duration: 114.771015ms)"],"step_count":1}
{"level":"info","ts":"2025-11-18T12:28:06.730906Z","caller":"traceutil/trace.go:172","msg":"trace[291273655] transaction","detail":"{read_only:false; number_of_response:1; response_revision:776; }","duration":"100.566963ms","start":"2025-11-18T12:28:06.630325Z","end":"2025-11-18T12:28:06.730892Z","steps":["trace[291273655] 'process raft request'  (duration: 100.543273ms)"],"step_count":1}
{"level":"info","ts":"2025-11-18T12:28:06.731173Z","caller":"traceutil/trace.go:172","msg":"trace[633548274] transaction","detail":"{read_only:false; response_revision:776; number_of_response:1; }","duration":"101.054595ms","start":"2025-11-18T12:28:06.630111Z","end":"2025-11-18T12:28:06.731165Z","steps":["trace[633548274] 'process raft request'  (duration: 100.69141ms)"],"step_count":1}
{"level":"info","ts":"2025-11-18T12:28:06.755777Z","caller":"traceutil/trace.go:172","msg":"trace[1267312038] range","detail":"{range_begin:/registry/statefulsets; range_end:; response_count:0; response_revision:775; }","duration":"116.792466ms","start":"2025-11-18T12:28:06.615936Z","end":"2025-11-18T12:28:06.732728Z","steps":["trace[1267312038] 'agreement among raft nodes before linearized reading'  (duration: 111.175071ms)"],"step_count":1}
{"level":"warn","ts":"2025-11-18T12:28:22.817611Z","caller":"wal/wal.go:845","msg":"slow fdatasync","took":"1.001315638s","expected-duration":"1s"}
{"level":"info","ts":"2025-11-18T12:28:25.453953Z","caller":"mvcc/index.go:194","msg":"compact tree index","revision":648}
{"level":"info","ts":"2025-11-18T12:28:25.492802Z","caller":"mvcc/kvstore_compaction.go:70","msg":"finished scheduled compaction","compact-revision":648,"took":"31.908607ms","hash":1901628947,"current-db-size-bytes":1495040,"current-db-size":"1.5 MB","current-db-size-in-use-bytes":1495040,"current-db-size-in-use":"1.5 MB"}
{"level":"info","ts":"2025-11-18T12:28:25.493089Z","caller":"mvcc/hash.go:157","msg":"storing new hash","hash":1901628947,"revision":648,"compact-revision":-1}


==> kernel <==
 12:28:39 up  1:09,  0 users,  load average: 8.80, 5.30, 3.76
Linux minikube 6.14.0-35-generic #35~24.04.1-Ubuntu SMP PREEMPT_DYNAMIC Tue Oct 14 13:55:17 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [f8a63359a8b0] <==
I1118 12:18:28.292894       1 autoregister_controller.go:144] Starting autoregister controller
I1118 12:18:28.292904       1 cache.go:32] Waiting for caches to sync for autoregister controller
I1118 12:18:28.292911       1 cache.go:39] Caches are synced for autoregister controller
I1118 12:18:28.323928       1 cache.go:39] Caches are synced for RemoteAvailability controller
I1118 12:18:28.324648       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I1118 12:18:28.325861       1 shared_informer.go:356] "Caches are synced" controller="configmaps"
I1118 12:18:28.329535       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I1118 12:18:28.329546       1 shared_informer.go:356] "Caches are synced" controller="cluster_authentication_trust_controller"
I1118 12:18:28.328581       1 shared_informer.go:356] "Caches are synced" controller="node_authorizer"
I1118 12:18:28.339274       1 cache.go:39] Caches are synced for LocalAvailability controller
I1118 12:18:28.346052       1 apf_controller.go:382] Running API Priority and Fairness config worker
I1118 12:18:28.346162       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I1118 12:18:28.353553       1 shared_informer.go:356] "Caches are synced" controller="ipallocator-repair-controller"
I1118 12:18:28.361336       1 shared_informer.go:356] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I1118 12:18:28.361609       1 policy_source.go:240] refreshing policies
E1118 12:18:28.395834       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E1118 12:18:28.398986       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I1118 12:18:28.452520       1 controller.go:667] quota admission added evaluator for: namespaces
I1118 12:18:28.455979       1 default_servicecidr_controller.go:228] Setting default ServiceCIDR condition Ready to True
I1118 12:18:28.461373       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 12:18:28.545479       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 12:18:28.548979       1 default_servicecidr_controller.go:137] Shutting down kubernetes-service-cidr-controller
I1118 12:18:29.236847       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I1118 12:18:29.321913       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I1118 12:18:29.423526       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I1118 12:18:29.427555       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I1118 12:18:32.049285       1 controller.go:667] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I1118 12:18:32.232269       1 controller.go:667] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I1118 12:18:32.694683       1 alloc.go:328] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W1118 12:18:32.724487       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I1118 12:18:32.729160       1 controller.go:667] quota admission added evaluator for: endpoints
I1118 12:18:32.741307       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I1118 12:18:33.231597       1 controller.go:667] quota admission added evaluator for: serviceaccounts
E1118 12:18:33.676733       1 writers.go:123] "Unhandled Error" err="apiserver was unable to write a JSON response: http: Handler timeout" logger="UnhandledError"
E1118 12:18:33.677083       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"http: Handler timeout\"}: http: Handler timeout" logger="UnhandledError"
{"level":"warn","ts":"2025-11-18T12:18:33.677141Z","logger":"etcd-client","caller":"v3@v3.6.4/retry_interceptor.go:65","msg":"retrying of unary invoker failed","target":"etcd-endpoints://0xc0016b8d20/127.0.0.1:2379","method":"/etcdserverpb.KV/Txn","attempt":0,"error":"rpc error: code = Canceled desc = context canceled"}
E1118 12:18:33.679446       1 writers.go:136] "Unhandled Error" err="apiserver was unable to write a fallback JSON response: http: Handler timeout" logger="UnhandledError"
E1118 12:18:33.682429       1 timeout.go:140] "Post-timeout activity" logger="UnhandledError" timeElapsed="5.491985ms" method="PATCH" path="/api/v1/namespaces/kube-system/pods/kube-scheduler-minikube/status" result=null
E1118 12:18:33.682829       1 finisher.go:175] "Unhandled Error" err="FinishRequest: post-timeout activity - time-elapsed: 5.712276ms, panicked: false, err: context canceled, panic-reason: <nil>" logger="UnhandledError"
I1118 12:18:33.910221       1 controller.go:667] quota admission added evaluator for: deployments.apps
I1118 12:18:33.989942       1 alloc.go:328] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I1118 12:18:34.049422       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I1118 12:18:38.407151       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 12:18:38.418637       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I1118 12:18:39.074733       1 controller.go:667] quota admission added evaluator for: controllerrevisions.apps
I1118 12:18:39.150575       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I1118 12:19:33.942395       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:19:47.233729       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:20:52.345165       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:20:52.377791       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:22:14.053933       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:22:19.646283       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:23:26.684223       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:23:45.940504       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:24:42.340141       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:24:57.255974       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:26:00.161020       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:28:05.937843       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:28:06.030624       1 stats.go:136] "Error getting keys" err="empty key: \"\""
I1118 12:28:28.277249       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-controller-manager [744869ea363f] <==
I1118 12:18:38.119907       1 shared_informer.go:349] "Waiting for caches to sync" controller="crt configmap"
I1118 12:18:38.126595       1 shared_informer.go:349] "Waiting for caches to sync" controller="resource quota"
I1118 12:18:38.229344       1 shared_informer.go:349] "Waiting for caches to sync" controller="garbage collector"
I1118 12:18:38.286487       1 shared_informer.go:356] "Caches are synced" controller="expand"
I1118 12:18:38.300134       1 shared_informer.go:356] "Caches are synced" controller="service-cidr-controller"
I1118 12:18:38.304687       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I1118 12:18:38.305479       1 shared_informer.go:356] "Caches are synced" controller="TTL"
I1118 12:18:38.306761       1 shared_informer.go:356] "Caches are synced" controller="TTL after finished"
I1118 12:18:38.314731       1 shared_informer.go:356] "Caches are synced" controller="disruption"
I1118 12:18:38.318949       1 shared_informer.go:356] "Caches are synced" controller="node"
I1118 12:18:38.319058       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I1118 12:18:38.319092       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I1118 12:18:38.319096       1 shared_informer.go:349] "Waiting for caches to sync" controller="cidrallocator"
I1118 12:18:38.319389       1 shared_informer.go:356] "Caches are synced" controller="cidrallocator"
I1118 12:18:38.321414       1 shared_informer.go:356] "Caches are synced" controller="crt configmap"
I1118 12:18:38.323906       1 shared_informer.go:356] "Caches are synced" controller="PV protection"
I1118 12:18:38.326282       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrapproving"
I1118 12:18:38.327516       1 shared_informer.go:356] "Caches are synced" controller="service account"
I1118 12:18:38.328157       1 shared_informer.go:356] "Caches are synced" controller="daemon sets"
I1118 12:18:38.328399       1 shared_informer.go:356] "Caches are synced" controller="namespace"
I1118 12:18:38.328423       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1118 12:18:38.332094       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1118 12:18:38.334427       1 shared_informer.go:356] "Caches are synced" controller="taint"
I1118 12:18:38.335314       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I1118 12:18:38.336387       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I1118 12:18:38.336898       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I1118 12:18:38.337576       1 shared_informer.go:356] "Caches are synced" controller="attach detach"
I1118 12:18:38.340256       1 shared_informer.go:356] "Caches are synced" controller="ReplicationController"
I1118 12:18:38.345534       1 shared_informer.go:356] "Caches are synced" controller="ClusterRoleAggregator"
I1118 12:18:38.347293       1 shared_informer.go:356] "Caches are synced" controller="deployment"
I1118 12:18:38.352039       1 shared_informer.go:356] "Caches are synced" controller="legacy-service-account-token-cleaner"
I1118 12:18:38.355303       1 shared_informer.go:356] "Caches are synced" controller="taint-eviction-controller"
I1118 12:18:38.355888       1 shared_informer.go:356] "Caches are synced" controller="HPA"
I1118 12:18:38.359836       1 shared_informer.go:356] "Caches are synced" controller="endpoint"
I1118 12:18:38.360380       1 shared_informer.go:356] "Caches are synced" controller="ephemeral"
I1118 12:18:38.362897       1 shared_informer.go:356] "Caches are synced" controller="PVC protection"
I1118 12:18:38.366678       1 shared_informer.go:356] "Caches are synced" controller="cronjob"
I1118 12:18:38.366876       1 shared_informer.go:356] "Caches are synced" controller="GC"
I1118 12:18:38.382837       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice_mirroring"
I1118 12:18:38.383243       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I1118 12:18:38.384701       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I1118 12:18:38.387302       1 shared_informer.go:356] "Caches are synced" controller="validatingadmissionpolicy-status"
I1118 12:18:38.387431       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I1118 12:18:38.387477       1 shared_informer.go:356] "Caches are synced" controller="persistent volume"
I1118 12:18:38.387817       1 shared_informer.go:356] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I1118 12:18:38.392345       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I1118 12:18:38.393854       1 shared_informer.go:356] "Caches are synced" controller="endpoint_slice"
I1118 12:18:38.394167       1 shared_informer.go:356] "Caches are synced" controller="resource_claim"
I1118 12:18:38.395005       1 shared_informer.go:356] "Caches are synced" controller="ReplicaSet"
I1118 12:18:38.395025       1 shared_informer.go:356] "Caches are synced" controller="VAC protection"
I1118 12:18:38.395035       1 shared_informer.go:356] "Caches are synced" controller="bootstrap_signer"
I1118 12:18:38.395768       1 shared_informer.go:356] "Caches are synced" controller="stateful set"
I1118 12:18:38.394204       1 shared_informer.go:356] "Caches are synced" controller="resource quota"
I1118 12:18:38.400190       1 shared_informer.go:356] "Caches are synced" controller="job"
I1118 12:18:38.402105       1 shared_informer.go:356] "Caches are synced" controller="garbage collector"
I1118 12:18:38.402316       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I1118 12:18:38.402323       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
E1118 12:28:06.844175       1 node_lifecycle_controller.go:967] "Error updating node" err="Operation cannot be fulfilled on nodes \"minikube\": the object has been modified; please apply your changes to the latest version and try again" logger="node-lifecycle-controller" node="minikube"
I1118 12:28:06.938411       1 node_lifecycle_controller.go:1025] "Controller detected that all Nodes are not-Ready. Entering master disruption mode" logger="node-lifecycle-controller"
I1118 12:28:17.887880       1 node_lifecycle_controller.go:1044] "Controller detected that some Nodes are Ready. Exiting master disruption mode" logger="node-lifecycle-controller"


==> kube-proxy [4b0372da560b] <==
I1118 12:18:41.768091       1 server_linux.go:53] "Using iptables proxy"
I1118 12:18:42.019220       1 shared_informer.go:349] "Waiting for caches to sync" controller="node informer cache"
I1118 12:18:42.120038       1 shared_informer.go:356] "Caches are synced" controller="node informer cache"
I1118 12:18:42.120094       1 server.go:219] "Successfully retrieved NodeIPs" NodeIPs=["192.168.49.2"]
E1118 12:18:42.126671       1 server.go:256] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I1118 12:18:42.469328       1 server.go:265] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I1118 12:18:42.470332       1 server_linux.go:132] "Using iptables Proxier"
I1118 12:18:42.555099       1 proxier.go:242] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
I1118 12:18:42.570584       1 server.go:527] "Version info" version="v1.34.0"
I1118 12:18:42.571139       1 server.go:529] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1118 12:18:42.598541       1 config.go:200] "Starting service config controller"
I1118 12:18:42.598867       1 shared_informer.go:349] "Waiting for caches to sync" controller="service config"
I1118 12:18:42.606771       1 config.go:309] "Starting node config controller"
I1118 12:18:42.619117       1 shared_informer.go:349] "Waiting for caches to sync" controller="node config"
I1118 12:18:42.619142       1 shared_informer.go:356] "Caches are synced" controller="node config"
I1118 12:18:42.618016       1 config.go:403] "Starting serviceCIDR config controller"
I1118 12:18:42.619165       1 shared_informer.go:349] "Waiting for caches to sync" controller="serviceCIDR config"
I1118 12:18:42.617965       1 config.go:106] "Starting endpoint slice config controller"
I1118 12:18:42.647558       1 shared_informer.go:349] "Waiting for caches to sync" controller="endpoint slice config"
I1118 12:18:42.718791       1 shared_informer.go:356] "Caches are synced" controller="service config"
I1118 12:18:42.735860       1 shared_informer.go:356] "Caches are synced" controller="serviceCIDR config"
I1118 12:18:42.749047       1 shared_informer.go:356] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [67fdad5f3b4d] <==
I1118 12:18:25.296073       1 serving.go:386] Generated self-signed cert in-memory
W1118 12:18:28.364714       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W1118 12:18:28.367440       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W1118 12:18:28.368968       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W1118 12:18:28.369984       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I1118 12:18:28.546848       1 server.go:175] "Starting Kubernetes Scheduler" version="v1.34.0"
I1118 12:18:29.140003       1 server.go:177] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1118 12:18:29.185905       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I1118 12:18:29.189803       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I1118 12:18:29.190359       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
E1118 12:18:29.243250       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
I1118 12:18:29.190716       1 shared_informer.go:349] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E1118 12:18:29.318012       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1118 12:18:29.320847       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1118 12:18:29.321159       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1118 12:18:29.321216       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1118 12:18:29.321276       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1118 12:18:29.321338       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1118 12:18:29.321725       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
E1118 12:18:29.321809       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1118 12:18:29.321867       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1118 12:18:29.321902       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1118 12:18:29.321935       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1118 12:18:29.323548       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1118 12:18:29.323600       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1118 12:18:29.329530       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1118 12:18:29.336651       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1118 12:18:29.336806       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1118 12:18:29.343759       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1118 12:18:29.344272       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1118 12:18:30.080292       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PodDisruptionBudget"
E1118 12:18:30.193884       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError" reflector="runtime/asm_amd64.s:1700" type="*v1.ConfigMap"
E1118 12:18:30.294839       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolumeClaim"
E1118 12:18:30.342372       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Pod"
E1118 12:18:30.388165       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIStorageCapacity"
E1118 12:18:30.388604       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicaSet"
E1118 12:18:30.418385       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Node"
E1118 12:18:30.440661       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSINode"
E1118 12:18:30.507886       1 reflector.go:205] "Failed to watch" err="failed to list *v1.DeviceClass: deviceclasses.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"deviceclasses\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.DeviceClass"
E1118 12:18:30.635422       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceSlice: resourceslices.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceslices\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceSlice"
E1118 12:18:30.664432       1 reflector.go:205] "Failed to watch" err="failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.CSIDriver"
E1118 12:18:30.676559       1 reflector.go:205] "Failed to watch" err="failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.VolumeAttachment"
E1118 12:18:30.687421       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Service"
E1118 12:18:30.741637       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StatefulSet"
E1118 12:18:30.768282       1 reflector.go:205] "Failed to watch" err="failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.StorageClass"
E1118 12:18:30.770375       1 reflector.go:205] "Failed to watch" err="failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.PersistentVolume"
E1118 12:18:30.904705       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ResourceClaim: resourceclaims.resource.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"resourceclaims\" in API group \"resource.k8s.io\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ResourceClaim"
E1118 12:18:30.911100       1 reflector.go:205] "Failed to watch" err="failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.ReplicationController"
E1118 12:18:30.926533       1 reflector.go:205] "Failed to watch" err="failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError" reflector="k8s.io/client-go/informers/factory.go:160" type="*v1.Namespace"
I1118 12:18:33.222130       1 shared_informer.go:356] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kubelet <==
Nov 18 12:18:34 minikube kubelet[2153]: E1118 12:18:34.422381    2153 manager.go:513] "Failed to read data from checkpoint" err="checkpoint is not found" checkpoint="kubelet_internal_checkpoint"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.422583    2153 eviction_manager.go:189] "Eviction manager: starting control loop"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.422600    2153 container_log_manager.go:146] "Initializing container log rotate workers" workers=1 monitorPeriod="10s"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.424460    2153 plugin_manager.go:118] "Starting Kubelet Plugin Manager"
Nov 18 12:18:34 minikube kubelet[2153]: E1118 12:18:34.437777    2153 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.525093    2153 kubelet_node_status.go:75] "Attempting to register node" node="minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.611823    2153 kubelet_node_status.go:124] "Node was previously registered" node="minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.613177    2153 kubelet_node_status.go:78] "Successfully registered node" node="minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.617617    2153 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.623059    2153 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.624612    2153 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.628178    2153 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.655104    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656040    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656088    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656101    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656115    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656126    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656137    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656149    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/e3a36fac0ae701bc11fad0a6716eec2c-etcd-certs\") pod \"etcd-minikube\" (UID: \"e3a36fac0ae701bc11fad0a6716eec2c\") " pod="kube-system/etcd-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656158    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/8312b4cdc4b705c0e12f63794469cfad-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"8312b4cdc4b705c0e12f63794469cfad\") " pod="kube-system/kube-apiserver-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.656178    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.659513    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.659907    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.660498    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/3b51c8241e224d47681cce32ea99b407-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"3b51c8241e224d47681cce32ea99b407\") " pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.660531    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/e3a36fac0ae701bc11fad0a6716eec2c-etcd-data\") pod \"etcd-minikube\" (UID: \"e3a36fac0ae701bc11fad0a6716eec2c\") " pod="kube-system/etcd-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: I1118 12:18:34.660552    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/dc6cf0a7bcb54d1f95cecc4d7b6b7d67-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"dc6cf0a7bcb54d1f95cecc4d7b6b7d67\") " pod="kube-system/kube-scheduler-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: E1118 12:18:34.792132    2153 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: E1118 12:18:34.793562    2153 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: E1118 12:18:34.799575    2153 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Nov 18 12:18:34 minikube kubelet[2153]: E1118 12:18:34.806913    2153 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Nov 18 12:18:35 minikube kubelet[2153]: I1118 12:18:35.072487    2153 apiserver.go:52] "Watching apiserver"
Nov 18 12:18:35 minikube kubelet[2153]: I1118 12:18:35.237675    2153 desired_state_of_world_populator.go:154] "Finished populating initial desired state of world"
Nov 18 12:18:35 minikube kubelet[2153]: I1118 12:18:35.546940    2153 kubelet.go:3219] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Nov 18 12:18:35 minikube kubelet[2153]: E1118 12:18:35.613966    2153 kubelet.go:3221] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Nov 18 12:18:38 minikube kubelet[2153]: I1118 12:18:38.768683    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-thnv5\" (UniqueName: \"kubernetes.io/projected/2d8ac0db-3391-4a41-a2a4-04b44129e3a4-kube-api-access-thnv5\") pod \"storage-provisioner\" (UID: \"2d8ac0db-3391-4a41-a2a4-04b44129e3a4\") " pod="kube-system/storage-provisioner"
Nov 18 12:18:38 minikube kubelet[2153]: I1118 12:18:38.768839    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/2d8ac0db-3391-4a41-a2a4-04b44129e3a4-tmp\") pod \"storage-provisioner\" (UID: \"2d8ac0db-3391-4a41-a2a4-04b44129e3a4\") " pod="kube-system/storage-provisioner"
Nov 18 12:18:39 minikube kubelet[2153]: I1118 12:18:39.379757    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/296aa8e1-2070-4eff-a542-e3a5dc2a3a48-xtables-lock\") pod \"kube-proxy-ghqs4\" (UID: \"296aa8e1-2070-4eff-a542-e3a5dc2a3a48\") " pod="kube-system/kube-proxy-ghqs4"
Nov 18 12:18:39 minikube kubelet[2153]: I1118 12:18:39.379800    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/296aa8e1-2070-4eff-a542-e3a5dc2a3a48-lib-modules\") pod \"kube-proxy-ghqs4\" (UID: \"296aa8e1-2070-4eff-a542-e3a5dc2a3a48\") " pod="kube-system/kube-proxy-ghqs4"
Nov 18 12:18:39 minikube kubelet[2153]: I1118 12:18:39.379815    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-8q698\" (UniqueName: \"kubernetes.io/projected/296aa8e1-2070-4eff-a542-e3a5dc2a3a48-kube-api-access-8q698\") pod \"kube-proxy-ghqs4\" (UID: \"296aa8e1-2070-4eff-a542-e3a5dc2a3a48\") " pod="kube-system/kube-proxy-ghqs4"
Nov 18 12:18:39 minikube kubelet[2153]: I1118 12:18:39.379829    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/296aa8e1-2070-4eff-a542-e3a5dc2a3a48-kube-proxy\") pod \"kube-proxy-ghqs4\" (UID: \"296aa8e1-2070-4eff-a542-e3a5dc2a3a48\") " pod="kube-system/kube-proxy-ghqs4"
Nov 18 12:18:39 minikube kubelet[2153]: E1118 12:18:39.475586    2153 reflector.go:205] "Failed to watch" err="failed to list *v1.ConfigMap: configmaps \"coredns\" is forbidden: User \"system:node:minikube\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" logger="UnhandledError" reflector="object-\"kube-system\"/\"coredns\"" type="*v1.ConfigMap"
Nov 18 12:18:39 minikube kubelet[2153]: E1118 12:18:39.486136    2153 status_manager.go:1018] "Failed to get status for pod" err="pods \"coredns-66bc5c9577-bcv8b\" is forbidden: User \"system:node:minikube\" cannot get resource \"pods\" in API group \"\" in the namespace \"kube-system\": no relationship found between node 'minikube' and this object" podUID="58a6147d-779b-4ce5-8518-1d1f2fe5e4f8" pod="kube-system/coredns-66bc5c9577-bcv8b"
Nov 18 12:18:39 minikube kubelet[2153]: I1118 12:18:39.585396    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/58a6147d-779b-4ce5-8518-1d1f2fe5e4f8-config-volume\") pod \"coredns-66bc5c9577-bcv8b\" (UID: \"58a6147d-779b-4ce5-8518-1d1f2fe5e4f8\") " pod="kube-system/coredns-66bc5c9577-bcv8b"
Nov 18 12:18:39 minikube kubelet[2153]: I1118 12:18:39.585502    2153 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-5kgl8\" (UniqueName: \"kubernetes.io/projected/58a6147d-779b-4ce5-8518-1d1f2fe5e4f8-kube-api-access-5kgl8\") pod \"coredns-66bc5c9577-bcv8b\" (UID: \"58a6147d-779b-4ce5-8518-1d1f2fe5e4f8\") " pod="kube-system/coredns-66bc5c9577-bcv8b"
Nov 18 12:18:39 minikube kubelet[2153]: I1118 12:18:39.741962    2153 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6f7bcfd98066f1c9f5b88b87921c4ca95f15591b4e46cefbbf9fee824f3dabc8"
Nov 18 12:18:40 minikube kubelet[2153]: I1118 12:18:40.865934    2153 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=3.865913697 podStartE2EDuration="3.865913697s" podCreationTimestamp="2025-11-18 12:18:37 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-18 12:18:40.863860918 +0000 UTC m=+7.145296681" watchObservedRunningTime="2025-11-18 12:18:40.865913697 +0000 UTC m=+7.147349457"
Nov 18 12:18:41 minikube kubelet[2153]: I1118 12:18:41.114521    2153 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="54a80b4ab01d264e34d0f2cccbf3e1f46fe7d14e4221af4c83c249e6983f79dd"
Nov 18 12:18:42 minikube kubelet[2153]: I1118 12:18:42.435555    2153 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-66bc5c9577-bcv8b" podStartSLOduration=3.435533267 podStartE2EDuration="3.435533267s" podCreationTimestamp="2025-11-18 12:18:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-18 12:18:42.389737729 +0000 UTC m=+8.671173489" watchObservedRunningTime="2025-11-18 12:18:42.435533267 +0000 UTC m=+8.716969045"
Nov 18 12:18:43 minikube kubelet[2153]: I1118 12:18:43.396608    2153 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Nov 18 12:18:44 minikube kubelet[2153]: I1118 12:18:44.884757    2153 kuberuntime_manager.go:1828] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Nov 18 12:18:44 minikube kubelet[2153]: I1118 12:18:44.888974    2153 kubelet_network.go:47] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Nov 18 12:18:48 minikube kubelet[2153]: I1118 12:18:48.686689    2153 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Nov 18 12:19:02 minikube kubelet[2153]: I1118 12:19:02.987454    2153 scope.go:117] "RemoveContainer" containerID="10a75222f40cd056adbd8235f1d7c697427c6ca102cf4a88013feeb6b4180878"
Nov 18 12:19:03 minikube kubelet[2153]: I1118 12:19:03.086249    2153 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-ghqs4" podStartSLOduration=24.086230957 podStartE2EDuration="24.086230957s" podCreationTimestamp="2025-11-18 12:18:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-11-18 12:18:42.631893333 +0000 UTC m=+8.913329093" watchObservedRunningTime="2025-11-18 12:19:03.086230957 +0000 UTC m=+29.367666715"
Nov 18 12:28:17 minikube kubelet[2153]: E1118 12:28:04.791626    2153 kubelet.go:2451] "Skipping pod synchronization" err="container runtime is down"
Nov 18 12:28:17 minikube kubelet[2153]: E1118 12:28:04.929484    2153 kubelet.go:2451] "Skipping pod synchronization" err="container runtime is down"
Nov 18 12:28:17 minikube kubelet[2153]: E1118 12:28:05.317241    2153 kubelet.go:2451] "Skipping pod synchronization" err="container runtime is down"
Nov 18 12:28:17 minikube kubelet[2153]: I1118 12:28:05.705930    2153 setters.go:543] "Node became not ready" node="minikube" condition={"type":"Ready","status":"False","lastHeartbeatTime":"2025-11-18T12:28:05Z","lastTransitionTime":"2025-11-18T12:28:05Z","reason":"KubeletNotReady","message":"container runtime is down"}
Nov 18 12:28:17 minikube kubelet[2153]: E1118 12:28:05.728163    2153 kubelet.go:2451] "Skipping pod synchronization" err="container runtime is down"


==> storage-provisioner [10a75222f40c] <==
I1118 12:18:40.673001       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1118 12:19:01.704457       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [93867fa34e98] <==
W1118 12:25:34.504709       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:34.532431       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:36.544635       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:36.574381       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:38.587056       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:38.598585       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:40.607208       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:40.624344       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:42.642486       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:42.658396       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:44.670772       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:44.683028       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:46.709023       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:46.736697       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:48.750523       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:48.759074       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:50.766341       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:50.799941       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:52.807877       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:52.821740       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:54.832529       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:54.852136       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:56.868934       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:56.885528       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:58.899199       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:25:58.915640       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:26:00.928098       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:26:00.945977       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:06.798031       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:06.907410       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:08.920821       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:08.933026       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:10.985557       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:11.038394       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:13.049788       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:13.300723       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:15.306520       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:15.352391       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:17.365079       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:17.399884       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:19.411382       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:19.427060       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:22.918364       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:22.959628       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:24.970416       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:24.989780       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:27.001899       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:27.022799       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:29.035565       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:29.056032       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:31.079568       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:31.090222       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:33.107684       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:33.121674       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:35.139747       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:35.165342       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:37.177744       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:37.198821       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:39.233122       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W1118 12:28:39.242474       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

